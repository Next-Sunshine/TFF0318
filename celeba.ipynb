{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "celeba.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "J0mdf9_RUA_8",
        "3axktlNhUYIL",
        "HnMoszHYUwp6",
        "EUP2B_P4VGjw",
        "3OkiyC6NXhXJ",
        "II64OFFjXmXg"
      ],
      "authorship_tag": "ABX9TyOojgf6+jzsZFKY4K5QFBkF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Next-Sunshine/TFF0318/blob/master/celeba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHbZdYrZRdfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76XflIxdTEqJ",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess:metadata_to_json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8_d2HIBTKGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyW0RdOgTcIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TARGET_NAME = 'Smiling'\n",
        "parent_path = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEFl-st_TeZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#从.txt文件中获取标注的元数据\n",
        "#返回值是每行变成的数组identities（[000001.jpg 2880...]）和attributes([000001.jpg -1  1  1 -1 -1 -1 -1...])\n",
        "def get_metadata():\n",
        "  #identity里面有一些数字2880之类的，是什么意思？身份？每一个identity是这样的：000001.jpg 2880\n",
        "\tf_identities = open(os.path.join(\n",
        "\t\tparent_path, 'data', 'raw', 'identity_CelebA.txt'), 'r')\n",
        "  #split()拆分字符串，通过指定分隔符对字符串进行切片，并返回分割后的字符串列表\n",
        "\tidentities = f_identities.read().split('\\n')\n",
        "\n",
        "  #attributes里面标注的是一些特征，-1代表否，1代表是\n",
        "\tf_attributes = open(os.path.join(\n",
        "\t\tparent_path, 'data', 'raw', 'list_attr_celeba.txt'), 'r')\n",
        "\tattributes = f_attributes.read().split('\\n')\n",
        "\n",
        "\treturn identities, attributes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrsYNEv0Tgni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#此处是identities是很多个000001.jpg 2880，是一个列表\n",
        "#返回值是字典，以名人的编号为索引，内容是图片名字列表{celeb_id:[images]}\n",
        "def get_celebrities_and_images(identities):\n",
        "  #字典，存放{celeb_id:[images]}\n",
        "\tall_celebs = {}\n",
        "\n",
        "  #line是取读出的每一行，每一行是\n",
        "\tfor line in identities:\n",
        "    #split()拆分字符串，通过指定分隔符对字符串进行切片，并返回分割后的字符串列表\n",
        "    #不带参数时以空格进行分割：每个info应该是[000001.jpg,2880]这种\n",
        "\t\tinfo = line.split()\n",
        "    #len(info)小于2说明这一行的元素不对，那么接着对下一行进行操作\n",
        "\t\tif len(info) < 2:\n",
        "\t\t\tcontinue\n",
        "    #info[0]即000001.jpg是图片名字，info[1]是2880这里用的celebrity应该是这个名人的编号\n",
        "\t\timage, celeb = info[0], info[1]\n",
        "    #接下来将这个名人的图片放在字典中他对应的数组里面，即如果字典all_celebs以这个名人编号的项不存在\n",
        "    #那么就在这个位置创建一个空数组并把名人图片号（000001.jpg）放进去，如果字典里面有这个名人的索引就直接append\n",
        "\t\tif celeb not in all_celebs:\n",
        "\t\t\tall_celebs[celeb] = []\n",
        "\t\tall_celebs[celeb].append(image)\n",
        "\n",
        "  #这里就是数据集介绍说的那个忽略掉图片数少于5张的名人\n",
        "\tgood_celebs = {c: all_celebs[c] for c in all_celebs if len(all_celebs[c]) >= 5}\n",
        "\treturn good_celebs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-WGZcB6TkWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#顾名思义：根据图片获得名人,正确答案是将{id:[images]}->{image:cele_id}\n",
        "#itendities形态待定，猜测是{id:[images]}\n",
        "def _get_celebrities_by_image(identities):\n",
        "  #good_images一个字典{image:cele_id}\n",
        "\tgood_images = {}\n",
        "\tfor c in identities:\n",
        "\t\timages = identities[c]\n",
        "\t\tfor img in images:\n",
        "\t\t\tgood_images[img] = c\n",
        "\treturn good_images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ-lMfgPTmXm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#输入是celebrities即{celeb_id:[images]}字典，attributes是list_attr_celeb.txt按行分割做成的列表\n",
        "#返回值是celeb_attributes即{celeb_id:[att]}字典，每个att都是images是否在attribute_name积极，也就是说如果图片微笑就是1，图片不微笑就是0\n",
        "#TARGET_NAME是smiling\n",
        "def get_celebrities_and_target(celebrities, attributes, attribute_name=TARGET_NAME):\n",
        "  #列名字，在list_attr_celeb.txt中是在第2行，所以是attributes[1],第一行是202599，是一共有多少张图片\n",
        "\tcol_names = attributes[1]\n",
        "  #col_idx是smiling所在的下标位置，先将列名根据空格分隔开，然后找到smiling的下标\n",
        "\tcol_idx = col_names.split().index(attribute_name)\n",
        "\n",
        "  #celeb_attributes是名人{celeb_id:[att是否微笑]}字典，其中微笑值和celebrities{celeb_id:[images]}中的image对应\n",
        "\tceleb_attributes = {}\n",
        "  #如果这里的celebrities指的是名人{id:[images]}字典\n",
        "  #good_images是{image:cele_id}\n",
        "\tgood_images = _get_celebrities_by_image(celebrities)\n",
        "\n",
        "  #从attributes[2]开始就是000001.jpg -1  1  1 -1 -1 -1 -1 -1 -1这样的格式，前面是图片id，后面是图片的特征\n",
        "\tfor line in attributes[2:]:\n",
        "    #将每行按照空格分割\n",
        "\t\tinfo = line.split()\n",
        "    #如果本行分割得到的元素一个都没有，那么说明是无意间键入了空格，接着分割下一行（我真聪明）\n",
        "\t\tif len(info) == 0:\n",
        "\t\t\tcontinue\n",
        "\n",
        "    #image是指图片的id(就是00001.jpg这种名字)，info[0]是获得列表中的第一项也就是图片名字\n",
        "    #如果图片名字没有出现在good_images({image:cele_id})中，则没有做下去的意义，continue\n",
        "\t\timage = info[0]\n",
        "\t\tif image not in good_images:\n",
        "\t\t\tcontinue\n",
        "\t\t\n",
        "    #celeb是名人的id号，根据image在good_images({image:cele_id})字典中获得对应的名人id号\n",
        "\t\tceleb = good_images[image]\n",
        "    #info[1:][col_idx]是先将代表属性的列切片出来，然后看smiling那一列是什么，int是转成int类型\n",
        "    #如果smiling对应的值是-1那么+1并除以2以后是0，如果是1那么+1除以2以后是1,\n",
        "    #所以att就是在指示这个图片有没有微笑\n",
        "\t\tatt = (int(info[1:][col_idx]) + 1) / 2\n",
        "\t\t\n",
        "    #如果名人celeb不在celeb_attributes这个字典中，那么和之前一样创建一个数组，并将att微笑值append进去\n",
        "\t\tif celeb not in celeb_attributes:\n",
        "\t\t\tceleb_attributes[celeb] = []\n",
        "\n",
        "\t\tceleb_attributes[celeb].append(att)\n",
        "\n",
        "\treturn celeb_attributes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaNLkuQ1To1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#获得名人id以及该名人样本数，以及每张图片是否微笑\n",
        "#celebrities应该是{celeb_id:[images]},targets应该是{celeb_id:[attr]}(att是图片中的名人是否微笑)\n",
        "#返回值all_data={'users':[celeb_id],'num_samples':[num_samples],'user_data':{'celeb_id':{'x':[images],'y':[attr]}} }\n",
        "def build_json_format(celebrities, targets):\n",
        "\tall_data = {}\n",
        "\n",
        "  #celeb_keys是在获得名人的id数组\n",
        "\tceleb_keys = [c for c in celebrities]\n",
        "  #num_samples是在获得每个名人有多少张图片数组\n",
        "\tnum_samples = [len(celebrities[c]) for c in celeb_keys]\n",
        "  #data是在做一个字典，索引是celeb_id，值是字典{'x':[images]，'y':[attr]}\n",
        "\tdata = {c: {'x': celebrities[c], 'y': targets[c]} for c in celebrities}\n",
        "\n",
        "  #格式all_data={'users':[celeb_id],'num_samples':[num_samples],'user_data':{'celeb_id':{'x':[images],'y':[attr]}} }\n",
        "\tall_data['users'] = celeb_keys\n",
        "\tall_data['num_samples'] = num_samples\n",
        "\tall_data['user_data'] = data\n",
        "\treturn all_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5KrZZMvTq-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#这是在把json数据写到文件夹里面\n",
        "def write_json(json_data):\n",
        "\tfile_name = 'all_data.json'\n",
        "  #parent_path一开始就有定义\n",
        "\tdir_path = os.path.join(parent_path, 'data', 'all_data')\n",
        "\n",
        "  #如果文件名不存在就新建\n",
        "\tif not os.path.exists(dir_path):\n",
        "\t\tos.mkdir(dir_path)\n",
        "\n",
        "  #合成最终文件存放的路径\n",
        "\tfile_path = os.path.join(dir_path, file_name)\n",
        "\n",
        "\tprint('writing {}'.format(file_name))\n",
        "\twith open(file_path, 'w') as outfile:\n",
        "\t\tjson.dump(json_data, outfile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivvrb59PTtO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def celeb_preprocess():\n",
        "  #从.txt文件中获取元数据,\n",
        "\tidentities, attributes = get_metadata()\n",
        "  #获得以名人id为索引内容为图片的字典{celeb_id:[images]}\n",
        "\tcelebrities = get_celebrities_and_images(identities)\n",
        "  #获得目标字典{celeb_id:[att]}\n",
        "\ttargets = get_celebrities_and_target(celebrities, attributes)\n",
        "  \n",
        "  #获得最终的json形式的数据并写入指定位置\n",
        "  #json_data={'users':[celeb_id],'num_samples':[num_samples],'user_data':{'celeb_id':{'x':[images],'y':[attr]}} }\n",
        "\tjson_data = build_json_format(celebrities, targets)\n",
        "\twrite_json(json_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0mdf9_RUA_8",
        "colab_type": "text"
      },
      "source": [
        "### cnn.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQkD7qn8UDV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from model import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY8WHzgiUF7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMAGE_SIZE = 84\n",
        "IMAGES_DIR = os.path.join('..', 'data', 'celeba', 'data', 'raw', 'img_align_celeba')\n",
        "\n",
        "class ClientModel(Model):\n",
        "    def __init__(self, seed, lr, num_classes):\n",
        "        self.num_classes = num_classes\n",
        "        super(ClientModel, self).__init__(seed, lr)\n",
        "\n",
        "    def create_model(self):\n",
        "        input_ph = tf.placeholder(\n",
        "            tf.float32, shape=(None, IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "        out = input_ph\n",
        "        for _ in range(4):\n",
        "            out = tf.layers.conv2d(out, 32, 3, padding='same')\n",
        "            out = tf.layers.batch_normalization(out, training=True)\n",
        "            out = tf.layers.max_pooling2d(out, 2, 2, padding='same')\n",
        "            out = tf.nn.relu(out)\n",
        "        out = tf.reshape(out, (-1, int(np.prod(out.get_shape()[1:]))))\n",
        "        logits = tf.layers.dense(out, self.num_classes)\n",
        "        label_ph = tf.placeholder(tf.int64, shape=(None,))\n",
        "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            labels=label_ph,\n",
        "            logits=logits)\n",
        "        predictions = tf.argmax(logits, axis=-1)\n",
        "        minimize_op = self.optimizer.minimize(\n",
        "            loss=loss, global_step=tf.train.get_global_step())\n",
        "        eval_metric_ops = tf.count_nonzero(\n",
        "            tf.equal(label_ph, tf.argmax(input=logits, axis=1)))\n",
        "        return input_ph, label_ph, minimize_op, eval_metric_ops, tf.math.reduce_mean(loss)\n",
        "\n",
        "    def process_x(self, raw_x_batch):\n",
        "        x_batch = [self._load_image(i) for i in raw_x_batch]\n",
        "        x_batch = np.array(x_batch)\n",
        "        return x_batch\n",
        "\n",
        "    def process_y(self, raw_y_batch):\n",
        "        return raw_y_batch\n",
        "\n",
        "    def _load_image(self, img_name):\n",
        "        img = Image.open(os.path.join(IMAGES_DIR, img_name))\n",
        "        img = img.resize((IMAGE_SIZE, IMAGE_SIZE)).convert('RGB')\n",
        "        return np.array(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3axktlNhUYIL",
        "colab_type": "text"
      },
      "source": [
        "### server.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSE3eIf-UIl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from baseline_constants import BYTES_WRITTEN_KEY, BYTES_READ_KEY, LOCAL_COMPUTATIONS_KEY\n",
        "\n",
        "class Server:\n",
        "    \n",
        "    def __init__(self, client_model):\n",
        "        self.client_model = client_model\n",
        "        self.model = client_model.get_params()\n",
        "        self.selected_clients = []\n",
        "        self.updates = []\n",
        "\n",
        "    def select_clients(self, my_round, possible_clients, num_clients=20):\n",
        "        \"\"\"Selects num_clients clients randomly from possible_clients.\n",
        "        \n",
        "        Note that within function, num_clients is set to\n",
        "            min(num_clients, len(possible_clients)).\n",
        "        Args:\n",
        "            possible_clients: Clients from which the server can select.\n",
        "            num_clients: Number of clients to select; default 20\n",
        "        Return:\n",
        "            list of (num_train_samples, num_test_samples)\n",
        "        \"\"\"\n",
        "        num_clients = min(num_clients, len(possible_clients))\n",
        "        np.random.seed(my_round)\n",
        "        self.selected_clients = np.random.choice(possible_clients, num_clients, replace=False)\n",
        "\n",
        "        return [(c.num_train_samples, c.num_test_samples) for c in self.selected_clients]\n",
        "\n",
        "    def train_model(self, num_epochs=1, batch_size=10, minibatch=None, clients=None):\n",
        "        \"\"\"Trains self.model on given clients.\n",
        "        \n",
        "        Trains model on self.selected_clients if clients=None;\n",
        "        each client's data is trained with the given number of epochs\n",
        "        and batches.\n",
        "        Args:\n",
        "            clients: list of Client objects.\n",
        "            num_epochs: Number of epochs to train.\n",
        "            batch_size: Size of training batches.\n",
        "            minibatch: fraction of client's data to apply minibatch sgd,\n",
        "                None to use FedAvg\n",
        "        Return:\n",
        "            bytes_written: number of bytes written by each client to server \n",
        "                dictionary with client ids as keys and integer values.\n",
        "            client computations: number of FLOPs computed by each client\n",
        "                dictionary with client ids as keys and integer values.\n",
        "            bytes_read: number of bytes read by each client from server\n",
        "                dictionary with client ids as keys and integer values.\n",
        "        \"\"\"\n",
        "        if clients is None:\n",
        "            clients = self.selected_clients\n",
        "        sys_metrics = {\n",
        "            c.id: {BYTES_WRITTEN_KEY: 0,\n",
        "                   BYTES_READ_KEY: 0,\n",
        "                   LOCAL_COMPUTATIONS_KEY: 0} for c in clients}\n",
        "        for c in clients:\n",
        "            c.model.set_params(self.model)\n",
        "            comp, num_samples, update = c.train(num_epochs, batch_size, minibatch)\n",
        "\n",
        "            sys_metrics[c.id][BYTES_READ_KEY] += c.model.size\n",
        "            sys_metrics[c.id][BYTES_WRITTEN_KEY] += c.model.size\n",
        "            sys_metrics[c.id][LOCAL_COMPUTATIONS_KEY] = comp\n",
        "\n",
        "            self.updates.append((num_samples, update))\n",
        "\n",
        "        return sys_metrics\n",
        "\n",
        "    def update_model(self):\n",
        "        total_weight = 0.\n",
        "        base = [0] * len(self.updates[0][1])\n",
        "        for (client_samples, client_model) in self.updates:\n",
        "            total_weight += client_samples\n",
        "            for i, v in enumerate(client_model):\n",
        "                base[i] += (client_samples * v.astype(np.float64))\n",
        "        averaged_soln = [v / total_weight for v in base]\n",
        "\n",
        "        self.model = averaged_soln\n",
        "        self.updates = []\n",
        "\n",
        "    def test_model(self, clients_to_test, set_to_use='test'):\n",
        "        \"\"\"Tests self.model on given clients.\n",
        "        Tests model on self.selected_clients if clients_to_test=None.\n",
        "        Args:\n",
        "            clients_to_test: list of Client objects.\n",
        "            set_to_use: dataset to test on. Should be in ['train', 'test'].\n",
        "        \"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        if clients_to_test is None:\n",
        "            clients_to_test = self.selected_clients\n",
        "\n",
        "        for client in clients_to_test:\n",
        "            client.model.set_params(self.model)\n",
        "            c_metrics = client.test(set_to_use)\n",
        "            metrics[client.id] = c_metrics\n",
        "        \n",
        "        return metrics\n",
        "\n",
        "    def get_clients_info(self, clients):\n",
        "        \"\"\"Returns the ids, hierarchies and num_samples for the given clients.\n",
        "        Returns info about self.selected_clients if clients=None;\n",
        "        Args:\n",
        "            clients: list of Client objects.\n",
        "        \"\"\"\n",
        "        if clients is None:\n",
        "            clients = self.selected_clients\n",
        "\n",
        "        ids = [c.id for c in clients]\n",
        "        groups = {c.id: c.group for c in clients}\n",
        "        num_samples = {c.id: c.num_samples for c in clients}\n",
        "        return ids, groups, num_samples\n",
        "\n",
        "    def save_model(self, path):\n",
        "        \"\"\"Saves the server model on checkpoints/dataset/model.ckpt.\"\"\"\n",
        "        # Save server model\n",
        "        self.client_model.set_params(self.model)\n",
        "        model_sess =  self.client_model.sess\n",
        "        return self.client_model.saver.save(model_sess, path)\n",
        "\n",
        "    def close_model(self):\n",
        "        self.client_model.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnMoszHYUwp6",
        "colab_type": "text"
      },
      "source": [
        "### client.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFV1riFvU15L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import warnings\n",
        "\n",
        "\n",
        "class Client:\n",
        "    \n",
        "    def __init__(self, client_id, group=None, train_data={'x' : [],'y' : []}, eval_data={'x' : [],'y' : []}, model=None):\n",
        "        self._model = model\n",
        "        self.id = client_id\n",
        "        self.group = group\n",
        "        self.train_data = train_data\n",
        "        self.eval_data = eval_data\n",
        "\n",
        "    def train(self, num_epochs=1, batch_size=10, minibatch=None):\n",
        "        \"\"\"Trains on self.model using the client's train_data.\n",
        "        Args:\n",
        "            num_epochs: Number of epochs to train. Unsupported if minibatch is provided (minibatch has only 1 epoch)\n",
        "            batch_size: Size of training batches.\n",
        "            minibatch: fraction of client's data to apply minibatch sgd,\n",
        "                None to use FedAvg\n",
        "        Return:\n",
        "            comp: number of FLOPs executed in training process\n",
        "            num_samples: number of samples used in training\n",
        "            update: set of weights\n",
        "            update_size: number of bytes in update\n",
        "        \"\"\"\n",
        "        if minibatch is None:\n",
        "            data = self.train_data\n",
        "            comp, update = self.model.train(data, num_epochs, batch_size)\n",
        "        else:\n",
        "            frac = min(1.0, minibatch)\n",
        "            num_data = max(1, int(frac*len(self.train_data[\"x\"])))\n",
        "            xs, ys = zip(*random.sample(list(zip(self.train_data[\"x\"], self.train_data[\"y\"])), num_data))\n",
        "            data = {'x': xs, 'y': ys}\n",
        "\n",
        "            # Minibatch trains for only 1 epoch - multiple local epochs don't make sense!\n",
        "            num_epochs = 1\n",
        "            comp, update = self.model.train(data, num_epochs, num_data)\n",
        "        num_train_samples = len(data['y'])\n",
        "        return comp, num_train_samples, update\n",
        "\n",
        "    def test(self, set_to_use='test'):\n",
        "        \"\"\"Tests self.model on self.test_data.\n",
        "        \n",
        "        Args:\n",
        "            set_to_use. Set to test on. Should be in ['train', 'test'].\n",
        "        Return:\n",
        "            dict of metrics returned by the model.\n",
        "        \"\"\"\n",
        "        assert set_to_use in ['train', 'test', 'val']\n",
        "        if set_to_use == 'train':\n",
        "            data = self.train_data\n",
        "        elif set_to_use == 'test' or set_to_use == 'val':\n",
        "            data = self.eval_data\n",
        "        return self.model.test(data)\n",
        "\n",
        "    @property\n",
        "    def num_test_samples(self):\n",
        "        \"\"\"Number of test samples for this client.\n",
        "        Return:\n",
        "            int: Number of test samples for this client\n",
        "        \"\"\"\n",
        "        if self.eval_data is None:\n",
        "            return 0\n",
        "        return len(self.eval_data['y'])\n",
        "\n",
        "    @property\n",
        "    def num_train_samples(self):\n",
        "        \"\"\"Number of train samples for this client.\n",
        "        Return:\n",
        "            int: Number of train samples for this client\n",
        "        \"\"\"\n",
        "        if self.train_data is None:\n",
        "            return 0\n",
        "        return len(self.train_data['y'])\n",
        "\n",
        "    @property\n",
        "    def num_samples(self):\n",
        "        \"\"\"Number samples for this client.\n",
        "        Return:\n",
        "            int: Number of samples for this client\n",
        "        \"\"\"\n",
        "        train_size = 0\n",
        "        if self.train_data is not None:\n",
        "            train_size = len(self.train_data['y'])\n",
        "\n",
        "        test_size = 0 \n",
        "        if self.eval_data is not  None:\n",
        "            test_size = len(self.eval_data['y'])\n",
        "        return train_size + test_size\n",
        "\n",
        "    @property\n",
        "    def model(self):\n",
        "        \"\"\"Returns this client reference to model being trained\"\"\"\n",
        "        return self._model\n",
        "\n",
        "    @model.setter\n",
        "    def model(self, model):\n",
        "        warnings.warn('The current implementation shares the model among all clients.'\n",
        "                      'Setting it on one client will effectively modify all clients.')\n",
        "        self._model = model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUP2B_P4VGjw",
        "colab_type": "text"
      },
      "source": [
        "### baseline_constants.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tFamU9wVJuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Configuration file for common models/experiments\"\"\"\n",
        "\n",
        "MAIN_PARAMS = { \n",
        "    'sent140': {\n",
        "        'small': (10, 2, 2),\n",
        "        'medium': (16, 2, 2),\n",
        "        'large': (24, 2, 2)\n",
        "        },\n",
        "    'femnist': {\n",
        "        'small': (30, 10, 2),\n",
        "        'medium': (100, 10, 2),\n",
        "        'large': (400, 20, 2)\n",
        "        },\n",
        "    'shakespeare': {\n",
        "        'small': (6, 2, 2),\n",
        "        'medium': (8, 2, 2),\n",
        "        'large': (20, 1, 2)\n",
        "        },\n",
        "    'celeba': {\n",
        "        'small': (30, 10, 2),\n",
        "        'medium': (100, 10, 2),\n",
        "        'large': (400, 20, 2)\n",
        "        },\n",
        "    'synthetic': {\n",
        "        'small': (6, 2, 2),\n",
        "        'medium': (8, 2, 2),\n",
        "        'large': (20, 1, 2)\n",
        "        },\n",
        "    'reddit': {\n",
        "        'small': (6, 2, 2),\n",
        "        'medium': (8, 2, 2),\n",
        "        'large': (20, 1, 2)\n",
        "        },\n",
        "}\n",
        "\"\"\"dict: Specifies execution parameters (tot_num_rounds, eval_every_num_rounds, clients_per_round)\"\"\"\n",
        "\n",
        "MODEL_PARAMS = {\n",
        "    'sent140.bag_dnn': (0.0003, 2), # lr, num_classes\n",
        "    'sent140.stacked_lstm': (0.0003, 25, 2, 100), # lr, seq_len, num_classes, num_hidden\n",
        "    'sent140.bag_log_reg': (0.0003, 2), # lr, num_classes\n",
        "    'femnist.cnn': (0.0003, 62), # lr, num_classes\n",
        "    'shakespeare.stacked_lstm': (0.0003, 80, 80, 256), # lr, seq_len, num_classes, num_hidden\n",
        "    'celeba.cnn': (0.1, 2), # lr, num_classes\n",
        "    'synthetic.log_reg': (0.0003, 5, 60), # lr, num_classes, input_dim\n",
        "    'reddit.stacked_lstm': (0.0003, 10, 256, 2), # lr, seq_len, num_hidden, num_layers\n",
        "}\n",
        "\"\"\"dict: Model specific parameter specification\"\"\"\n",
        "\n",
        "ACCURACY_KEY = 'accuracy'\n",
        "BYTES_WRITTEN_KEY = 'bytes_written'\n",
        "BYTES_READ_KEY = 'bytes_read'\n",
        "LOCAL_COMPUTATIONS_KEY = 'local_computations'\n",
        "NUM_ROUND_KEY = 'round_number'\n",
        "NUM_SAMPLES_KEY = 'num_samples'\n",
        "CLIENT_ID_KEY = 'client_id'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OkiyC6NXhXJ",
        "colab_type": "text"
      },
      "source": [
        "### model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB9i9LYzXjaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Interfaces for ClientModel and ServerModel.\"\"\"\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "from baseline_constants import ACCURACY_KEY\n",
        "\n",
        "from utils.model_utils import batch_data\n",
        "from utils.tf_utils import graph_size\n",
        "\n",
        "\n",
        "class Model(ABC):\n",
        "\n",
        "    def __init__(self, seed, lr, optimizer=None):\n",
        "        self.lr = lr\n",
        "        self.seed = seed\n",
        "        self._optimizer = optimizer\n",
        "\n",
        "        self.graph = tf.Graph()\n",
        "        with self.graph.as_default():\n",
        "            tf.set_random_seed(123 + self.seed)\n",
        "            self.features, self.labels, self.train_op, self.eval_metric_ops, self.loss = self.create_model()\n",
        "            self.saver = tf.train.Saver()\n",
        "        self.sess = tf.Session(graph=self.graph)\n",
        "\n",
        "        self.size = graph_size(self.graph)\n",
        "\n",
        "        with self.graph.as_default():\n",
        "            self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "            metadata = tf.RunMetadata()\n",
        "            opts = tf.profiler.ProfileOptionBuilder.float_operation()\n",
        "            self.flops = tf.profiler.profile(self.graph, run_meta=metadata, cmd='scope', options=opts).total_float_ops\n",
        "\n",
        "        np.random.seed(self.seed)\n",
        "\n",
        "    def set_params(self, model_params):\n",
        "        with self.graph.as_default():\n",
        "            all_vars = tf.trainable_variables()\n",
        "            for variable, value in zip(all_vars, model_params):\n",
        "                variable.load(value, self.sess)\n",
        "\n",
        "    def get_params(self):\n",
        "        with self.graph.as_default():\n",
        "            model_params = self.sess.run(tf.trainable_variables())\n",
        "        return model_params\n",
        "\n",
        "    @property\n",
        "    def optimizer(self):\n",
        "        \"\"\"Optimizer to be used by the model.\"\"\"\n",
        "        if self._optimizer is None:\n",
        "            self._optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.lr)\n",
        "\n",
        "        return self._optimizer\n",
        "\n",
        "    @abstractmethod\n",
        "    def create_model(self):\n",
        "        \"\"\"Creates the model for the task.\n",
        "        Returns:\n",
        "            A 4-tuple consisting of:\n",
        "                features: A placeholder for the samples' features.\n",
        "                labels: A placeholder for the samples' labels.\n",
        "                train_op: A Tensorflow operation that, when run with the features and\n",
        "                    the labels, trains the model.\n",
        "                eval_metric_ops: A Tensorflow operation that, when run with features and labels,\n",
        "                    returns the accuracy of the model.\n",
        "        \"\"\"\n",
        "        return None, None, None, None, None\n",
        "\n",
        "    def train(self, data, num_epochs=1, batch_size=10):\n",
        "        \"\"\"\n",
        "        Trains the client model.\n",
        "        Args:\n",
        "            data: Dict of the form {'x': [list], 'y': [list]}.\n",
        "            num_epochs: Number of epochs to train.\n",
        "            batch_size: Size of training batches.\n",
        "        Return:\n",
        "            comp: Number of FLOPs computed while training given data\n",
        "            update: List of np.ndarray weights, with each weight array\n",
        "                corresponding to a variable in the resulting graph\n",
        "        \"\"\"\n",
        "        for _ in range(num_epochs):\n",
        "            self.run_epoch(data, batch_size)\n",
        "\n",
        "        update = self.get_params()\n",
        "        comp = num_epochs * (len(data['y'])//batch_size) * batch_size * self.flops\n",
        "        return comp, update\n",
        "\n",
        "    def run_epoch(self, data, batch_size):\n",
        "\n",
        "        for batched_x, batched_y in batch_data(data, batch_size, seed=self.seed):\n",
        "            \n",
        "            input_data = self.process_x(batched_x)\n",
        "            target_data = self.process_y(batched_y)\n",
        "            \n",
        "            with self.graph.as_default():\n",
        "                self.sess.run(self.train_op,\n",
        "                    feed_dict={\n",
        "                        self.features: input_data,\n",
        "                        self.labels: target_data\n",
        "                    })\n",
        "\n",
        "    def test(self, data):\n",
        "        \"\"\"\n",
        "        Tests the current model on the given data.\n",
        "        Args:\n",
        "            data: dict of the form {'x': [list], 'y': [list]}\n",
        "        Return:\n",
        "            dict of metrics that will be recorded by the simulation.\n",
        "        \"\"\"\n",
        "        x_vecs = self.process_x(data['x'])\n",
        "        labels = self.process_y(data['y'])\n",
        "        with self.graph.as_default():\n",
        "            tot_acc, loss = self.sess.run(\n",
        "                [self.eval_metric_ops, self.loss],\n",
        "                feed_dict={self.features: x_vecs, self.labels: labels}\n",
        "            )\n",
        "        acc = float(tot_acc) / x_vecs.shape[0]\n",
        "        return {ACCURACY_KEY: acc, 'loss': loss}\n",
        "\n",
        "    def close(self):\n",
        "        self.sess.close()\n",
        "\n",
        "    @abstractmethod\n",
        "    def process_x(self, raw_x_batch):\n",
        "        \"\"\"Pre-processes each batch of features before being fed to the model.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def process_y(self, raw_y_batch):\n",
        "        \"\"\"Pre-processes each batch of labels before being fed to the model.\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class ServerModel:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    @property\n",
        "    def size(self):\n",
        "        return self.model.size\n",
        "\n",
        "    @property\n",
        "    def cur_model(self):\n",
        "        return self.model\n",
        "\n",
        "    def send_to(self, clients):\n",
        "        \"\"\"Copies server model variables to each of the given clients\n",
        "        Args:\n",
        "            clients: list of Client objects\n",
        "        \"\"\"\n",
        "        var_vals = {}\n",
        "        with self.model.graph.as_default():\n",
        "            all_vars = tf.trainable_variables()\n",
        "            for v in all_vars:\n",
        "                val = self.model.sess.run(v)\n",
        "                var_vals[v.name] = val\n",
        "        for c in clients:\n",
        "            with c.model.graph.as_default():\n",
        "                all_vars = tf.trainable_variables()\n",
        "                for v in all_vars:\n",
        "                    v.load(var_vals[v.name], c.model.sess)\n",
        "\n",
        "    def save(self, path='checkpoints/model.ckpt'):\n",
        "        return self.model.saver.save(self.model.sess, path)\n",
        "\n",
        "    def close(self):\n",
        "        self.model.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II64OFFjXmXg",
        "colab_type": "text"
      },
      "source": [
        "### main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArnGGONBXnxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Script to run the baselines.\"\"\"\n",
        "import argparse\n",
        "import importlib\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "import metrics.writer as metrics_writer\n",
        "\n",
        "from baseline_constants import MAIN_PARAMS, MODEL_PARAMS\n",
        "from client import Client\n",
        "from server import Server\n",
        "from model import ServerModel\n",
        "\n",
        "from utils.args import parse_args\n",
        "from utils.model_utils import read_data\n",
        "\n",
        "STAT_METRICS_PATH = 'metrics/stat_metrics.csv'\n",
        "SYS_METRICS_PATH = 'metrics/sys_metrics.csv'\n",
        "\n",
        "def main():\n",
        "\n",
        "    args = parse_args()\n",
        "    \n",
        "    # Set the random seed if provided (affects client sampling, and batching)\n",
        "    random.seed(1 + args.seed)\n",
        "    np.random.seed(12 + args.seed)\n",
        "    tf.set_random_seed(123 + args.seed)\n",
        "\n",
        "    model_path = '%s/%s.py' % (args.dataset, args.model)\n",
        "    if not os.path.exists(model_path):\n",
        "        print('Please specify a valid dataset and a valid model.')\n",
        "    model_path = '%s.%s' % (args.dataset, args.model)\n",
        "    \n",
        "    print('############################## %s ##############################' % model_path)\n",
        "    mod = importlib.import_module(model_path)\n",
        "    ClientModel = getattr(mod, 'ClientModel')\n",
        "\n",
        "    tup = MAIN_PARAMS[args.dataset][args.t]\n",
        "    num_rounds = args.num_rounds if args.num_rounds != -1 else tup[0]\n",
        "    eval_every = args.eval_every if args.eval_every != -1 else tup[1]\n",
        "    clients_per_round = args.clients_per_round if args.clients_per_round != -1 else tup[2]\n",
        "\n",
        "    # Suppress tf warnings\n",
        "    tf.logging.set_verbosity(tf.logging.WARN)\n",
        "\n",
        "    # Create 2 models\n",
        "    model_params = MODEL_PARAMS[model_path]\n",
        "    if args.lr != -1:\n",
        "        model_params_list = list(model_params)\n",
        "        model_params_list[0] = args.lr\n",
        "        model_params = tuple(model_params_list)\n",
        "\n",
        "    # Create client model, and share params with server model\n",
        "    tf.reset_default_graph()\n",
        "    client_model = ClientModel(args.seed, *model_params)\n",
        "\n",
        "    # Create server\n",
        "    server = Server(client_model)\n",
        "\n",
        "    # Create clients\n",
        "    clients = setup_clients(args.dataset, client_model, args.use_val_set)\n",
        "    client_ids, client_groups, client_num_samples = server.get_clients_info(clients)\n",
        "    print('Clients in Total: %d' % len(clients))\n",
        "\n",
        "    # Initial status\n",
        "    print('--- Random Initialization ---')\n",
        "    stat_writer_fn = get_stat_writer_function(client_ids, client_groups, client_num_samples, args)\n",
        "    sys_writer_fn = get_sys_writer_function(args)\n",
        "    print_stats(0, server, clients, client_num_samples, args, stat_writer_fn, args.use_val_set)\n",
        "\n",
        "    # Simulate training\n",
        "    for i in range(num_rounds):\n",
        "        print('--- Round %d of %d: Training %d Clients ---' % (i + 1, num_rounds, clients_per_round))\n",
        "\n",
        "        # Select clients to train this round\n",
        "        server.select_clients(i, online(clients), num_clients=clients_per_round)\n",
        "        c_ids, c_groups, c_num_samples = server.get_clients_info(server.selected_clients)\n",
        "\n",
        "        # Simulate server model training on selected clients' data\n",
        "        sys_metrics = server.train_model(num_epochs=args.num_epochs, batch_size=args.batch_size, minibatch=args.minibatch)\n",
        "        sys_writer_fn(i + 1, c_ids, sys_metrics, c_groups, c_num_samples)\n",
        "        \n",
        "        # Update server model\n",
        "        server.update_model()\n",
        "\n",
        "        # Test model\n",
        "        if (i + 1) % eval_every == 0 or (i + 1) == num_rounds:\n",
        "            print_stats(i + 1, server, clients, client_num_samples, args, stat_writer_fn, args.use_val_set)\n",
        "    \n",
        "    # Save server model\n",
        "    ckpt_path = os.path.join('checkpoints', args.dataset)\n",
        "    if not os.path.exists(ckpt_path):\n",
        "        os.makedirs(ckpt_path)\n",
        "    save_path = server.save_model(os.path.join(ckpt_path, '{}.ckpt'.format(args.model)))\n",
        "    print('Model saved in path: %s' % save_path)\n",
        "\n",
        "    # Close models\n",
        "    server.close_model()\n",
        "\n",
        "def online(clients):\n",
        "    \"\"\"We assume all users are always online.\"\"\"\n",
        "    return clients\n",
        "\n",
        "\n",
        "def create_clients(users, groups, train_data, test_data, model):\n",
        "    if len(groups) == 0:\n",
        "        groups = [[] for _ in users]\n",
        "    clients = [Client(u, g, train_data[u], test_data[u], model) for u, g in zip(users, groups)]\n",
        "    return clients\n",
        "\n",
        "\n",
        "def setup_clients(dataset, model=None, use_val_set=False):\n",
        "    \"\"\"Instantiates clients based on given train and test data directories.\n",
        "    Return:\n",
        "        all_clients: list of Client objects.\n",
        "    \"\"\"\n",
        "    eval_set = 'test' if not use_val_set else 'val'\n",
        "    train_data_dir = os.path.join('..', 'data', dataset, 'data', 'train')\n",
        "    test_data_dir = os.path.join('..', 'data', dataset, 'data', eval_set)\n",
        "\n",
        "    users, groups, train_data, test_data = read_data(train_data_dir, test_data_dir)\n",
        "\n",
        "    clients = create_clients(users, groups, train_data, test_data, model)\n",
        "\n",
        "    return clients\n",
        "\n",
        "\n",
        "def get_stat_writer_function(ids, groups, num_samples, args):\n",
        "\n",
        "    def writer_fn(num_round, metrics, partition):\n",
        "        metrics_writer.print_metrics(\n",
        "            num_round, ids, metrics, groups, num_samples, partition, args.metrics_dir, '{}_{}'.format(args.metrics_name, 'stat'))\n",
        "\n",
        "    return writer_fn\n",
        "\n",
        "\n",
        "def get_sys_writer_function(args):\n",
        "\n",
        "    def writer_fn(num_round, ids, metrics, groups, num_samples):\n",
        "        metrics_writer.print_metrics(\n",
        "            num_round, ids, metrics, groups, num_samples, 'train', args.metrics_dir, '{}_{}'.format(args.metrics_name, 'sys'))\n",
        "\n",
        "    return writer_fn\n",
        "\n",
        "\n",
        "def print_stats(\n",
        "    num_round, server, clients, num_samples, args, writer, use_val_set):\n",
        "    \n",
        "    train_stat_metrics = server.test_model(clients, set_to_use='train')\n",
        "    print_metrics(train_stat_metrics, num_samples, prefix='train_')\n",
        "    writer(num_round, train_stat_metrics, 'train')\n",
        "\n",
        "    eval_set = 'test' if not use_val_set else 'val'\n",
        "    test_stat_metrics = server.test_model(clients, set_to_use=eval_set)\n",
        "    print_metrics(test_stat_metrics, num_samples, prefix='{}_'.format(eval_set))\n",
        "    writer(num_round, test_stat_metrics, eval_set)\n",
        "\n",
        "\n",
        "def print_metrics(metrics, weights, prefix=''):\n",
        "    \"\"\"Prints weighted averages of the given metrics.\n",
        "    Args:\n",
        "        metrics: dict with client ids as keys. Each entry is a dict\n",
        "            with the metrics of that client.\n",
        "        weights: dict with client ids as keys. Each entry is the weight\n",
        "            for that client.\n",
        "    \"\"\"\n",
        "    ordered_weights = [weights[c] for c in sorted(weights)]\n",
        "    metric_names = metrics_writer.get_metrics_names(metrics)\n",
        "    to_ret = None\n",
        "    for metric in metric_names:\n",
        "        ordered_metric = [metrics[c][metric] for c in sorted(metrics)]\n",
        "        print('%s: %g, 10th percentile: %g, 50th percentile: %g, 90th percentile %g' \\\n",
        "              % (prefix + metric,\n",
        "                 np.average(ordered_metric, weights=ordered_weights),\n",
        "                 np.percentile(ordered_metric, 10),\n",
        "                 np.percentile(ordered_metric, 50),\n",
        "                 np.percentile(ordered_metric, 90)))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}