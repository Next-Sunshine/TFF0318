{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestForTFF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Next-Sunshine/TFF0318/blob/master/TestForTFF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9pTTi0lPh1w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "d761dc05-faa6-4462-fe6f-318b529feb85"
      },
      "source": [
        "#环境测试\n",
        "#@test {\"skip\":true}\n",
        "!pip install --quiet --upgrade tensorflow_federated\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▊                               | 10kB 23.1MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 22.3MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30kB 25.7MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 29.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51kB 31.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 61kB 33.9MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 71kB 34.6MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 81kB 35.3MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92kB 36.9MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102kB 38.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112kB 38.0MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 122kB 38.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 133kB 38.0MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 143kB 38.0MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 153kB 38.0MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 163kB 38.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174kB 38.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 184kB 38.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 194kB 38.0MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 204kB 38.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215kB 38.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 225kB 38.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 235kB 38.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 245kB 38.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 256kB 38.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 266kB 38.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 276kB 38.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 286kB 38.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 296kB 38.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 307kB 38.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 317kB 38.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 327kB 38.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 337kB 38.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 348kB 38.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 358kB 38.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 368kB 38.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 378kB 38.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 389kB 38.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 399kB 38.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 409kB 38.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 419kB 38.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 430kB 38.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 57.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 421.8MB 39kB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8MB 26.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 16.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 57.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 20.0MB 12.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 450kB 50.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.9MB 60.6MB/s \n",
            "\u001b[?25h  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITkxSUMiRa01",
        "colab_type": "code",
        "outputId": "b50abe8d-9df4-427b-9d06-9d80bb542728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "tf.compat.v1.enable_v2_behavior()\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "tff.federated_computation(lambda: 'Hello, world!')()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'Hello, world!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRJgOTiAR3T4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "362116f7-46a3-4082-ccd5-47f645f440ea"
      },
      "source": [
        "#装载数据集，实验使用EMNIST数据集\n",
        "emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()\n",
        "class MNISTLoader():\n",
        "    def __init__(self):\n",
        "        #emnist = tff.simulation.datasets.emnist\n",
        "        (self.train_data, self.train_label), (self.test_data, self.test_label) = emnist_train, emnist_test\n",
        "        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道\n",
        "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
        "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
        "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
        "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
        "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
        " \n",
        "    def get_batch(self, batch_size):\n",
        "        # 从数据集中随机取出batch_size个元素并返回\n",
        "        index = np.random.randint(0, np.shape(self.train_data)[0], batch_size)\n",
        "        return self.train_data[index, :], self.train_label[index]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tff-datasets-public/fed_emnist_digitsonly.tar.bz2\n",
            "97402880/97398400 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHXBNT_UxIWo",
        "colab_type": "code",
        "outputId": "43a99039-007d-4632-b325-510ce81a4edf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#此处得到客户端的总数K，因为每一轮随机选择C×K的向上取整\n",
        "#根据FedAvg做的研究C取0.2可获得较好的性能，为了简化模型K这里取固定的数\n",
        "K = len(emnist_train.client_ids)\n",
        "C = 0.2\n",
        "print(K)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc578aa2UuZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NUM_CLIENTS在实验的时候应该是K×C\n",
        "NUM_CLIENTS = 10\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 20\n",
        "SHUFFLE_BUFFER = 100\n",
        "PREFETCH_BUFFER = 10\n",
        "\n",
        "#预处理，将图片中的‘pixels'、‘label'分别表示成x和y\n",
        "#将28×28的图像展平成784个元素，打乱顺序\n",
        "def preprocess(dataset):\n",
        "  #内部函数将像素和标签转换成x和y，并将像素展平\n",
        "  def batch_format_fn(element):\n",
        "    return collections.OrderedDict(\n",
        "        x = tf.reshape(element['pixels'], [-1,28,28,1]),\n",
        "        #x = np.expand_dims(element['pixels'], axis=-1),\n",
        "        #x = element['pixels'],\n",
        "        y = tf.reshape(element['label'], [-1,1])\n",
        "    )\n",
        "  \n",
        "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRqmbbzamP1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#为指定用户创建联邦数据,接收训练集和用户id\n",
        "def make_federated_data(client_data, client_ids):\n",
        "  return [\n",
        "      preprocess(client_data.create_tf_dataset_for_client(x))\n",
        "      for x in client_ids\n",
        "    ]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDHJSC8OYaZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#这里改成随机获得NUM_CLIENTS个用户，模拟联邦学习每轮的随机选择用户\n",
        " sample_clients = random.sample(emnist_train.client_ids, NUM_CLIENTS)\n",
        "# sample_clients #说明随机用户的id是取出来了的\n",
        "# type(emnist_train.client_ids)  #client_ids是一个列表\n",
        "\n",
        "federated_train_data = make_federated_data(emnist_train, sample_clients)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZEcia5FltoP",
        "colab_type": "text"
      },
      "source": [
        "间隔界"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4MRt7FEh2gq",
        "colab_type": "code",
        "outputId": "7cf17d6d-983d-4309-919c-8ff493d20a02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "#！自己添加代码中，正在想办法把训练模型改成CNN\n",
        "class CNN(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(\n",
        "            filters=32,             # 卷积层神经元（卷积核）数目\n",
        "            kernel_size=[5, 5],     # 感受野大小\n",
        "            padding='same',         # padding策略（vaild 或 same）\n",
        "            activation=tf.nn.relu   # 激活函数\n",
        "        )\n",
        "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
        "        self.conv2 = tf.keras.layers.Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=[5, 5],\n",
        "            padding='same',\n",
        "            activation=tf.nn.relu\n",
        "        )\n",
        "        #print(\"kernel:\",kernel_size)\n",
        "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
        "        self.flatten = tf.keras.layers.Reshape(target_shape=(7 * 7 * 64,))\n",
        "        self.dense1 = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)\n",
        "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
        " \n",
        "    def call(self, inputs):\n",
        "        x = self.conv1(inputs)                  # [batch_size, 28, 28, 32]\n",
        "        x = self.pool1(x)                       # [batch_size, 14, 14, 32]\n",
        "        x = self.conv2(x)                       # [batch_size, 14, 14, 64]\n",
        "        x = self.pool2(x)                       # [batch_size, 7, 7, 64]\n",
        "        x = self.flatten(x)                     # [batch_size, 7 * 7 * 64]\n",
        "        x = self.dense1(x)                      # [batch_size, 1024]\n",
        "        x = self.dense2(x)\n",
        "        #tf.nn.softmax()将原始输出归一化，且能凸显原始向量中最大的值                      # [batch_size, 10]\n",
        "        output = tf.nn.softmax(x)\n",
        "        return output\n",
        "\"\"\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#！自己添加代码中，正在想办法把训练模型改成CNN\\nclass CNN(tf.keras.Model):\\n    def __init__(self):\\n        super().__init__()\\n        self.conv1 = tf.keras.layers.Conv2D(\\n            filters=32,             # 卷积层神经元（卷积核）数目\\n            kernel_size=[5, 5],     # 感受野大小\\n            padding=\\'same\\',         # padding策略（vaild 或 same）\\n            activation=tf.nn.relu   # 激活函数\\n        )\\n        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\\n        self.conv2 = tf.keras.layers.Conv2D(\\n            filters=64,\\n            kernel_size=[5, 5],\\n            padding=\\'same\\',\\n            activation=tf.nn.relu\\n        )\\n        #print(\"kernel:\",kernel_size)\\n        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\\n        self.flatten = tf.keras.layers.Reshape(target_shape=(7 * 7 * 64,))\\n        self.dense1 = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)\\n        self.dense2 = tf.keras.layers.Dense(units=10)\\n \\n    def call(self, inputs):\\n        x = self.conv1(inputs)                  # [batch_size, 28, 28, 32]\\n        x = self.pool1(x)                       # [batch_size, 14, 14, 32]\\n        x = self.conv2(x)                       # [batch_size, 14, 14, 64]\\n        x = self.pool2(x)                       # [batch_size, 7, 7, 64]\\n        x = self.flatten(x)                     # [batch_size, 7 * 7 * 64]\\n        x = self.dense1(x)                      # [batch_size, 1024]\\n        x = self.dense2(x)\\n        #tf.nn.softmax()将原始输出归一化，且能凸显原始向量中最大的值                      # [batch_size, 10]\\n        output = tf.nn.softmax(x)\\n        return output\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRwWOrazj5R_",
        "colab_type": "code",
        "outputId": "4b8eb1e7-b4ad-4f73-ccf7-391f6050c634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "#！此处也是自己添加代码\n",
        "num_epochs = 5\n",
        "batch_size = 50\n",
        "learning_rate = 0.001\n",
        "\n",
        "#实例化模型\n",
        "model = CNN()\n",
        "#data_loader = MNISTLoader()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "\"\"\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#！此处也是自己添加代码\\nnum_epochs = 5\\nbatch_size = 50\\nlearning_rate = 0.001\\n\\n#实例化模型\\nmodel = CNN()\\n#data_loader = MNISTLoader()\\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\\n\\nlearning_rate = 0.001\\noptimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw419GpLlmz9",
        "colab_type": "text"
      },
      "source": [
        "间隔界\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNo6O_5qSNSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#创建一个变量集合来表示所有变量,包括model(weights+bias)以及metrics(num_examples,loss_sum,accuracy_sum)\n",
        "# MnistVariables = collections.namedtuple(\n",
        "#     'MnistVariables','weights bias num_examples loss_sum accuracy_sum'\n",
        "# )\n",
        "MnistVariables = collections.namedtuple(\n",
        "    'MnistVariables','cnn_conv2d_kernel cnn_conv2d_bias cnn_conv2d_1_kernel cnn_conv2d_1_bias cnn_dense_kernel cnn_dense_bias cnn_dense_1_kernel cnn_dense_1_bias num_examples loss_sum accuracy_sum'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSEDLYxJTZ8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#创建变量并初始化\n",
        "def create_mnist_variables():\n",
        "  #核初始化函数，使用正态分布，先使用tf.XXX(初始参数)生成一个模型关键字C，再使用C(arg1)传参数进去\n",
        "  kernel_initalizer=tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05)\n",
        "  return MnistVariables(\n",
        "      #第一层卷积在5×5的patch中算出32个特征，权重张量形状为[5,5,1,32]\n",
        "      #前两个维度是patch的大小，接着是输入的通道数，最后是输出的通道数目\n",
        "      #h_conv1.shape=[-1,28,28,32]，第一个pooling层将其变成[-1,14,14,32]     \n",
        "      cnn_conv2d_kernel=tf.Variable(\n",
        "          #权重初始化至关重要，不能全部初始化为0！！！\n",
        "          initial_value=kernel_initalizer(shape=(5,5,1,32), dtype=tf.float32),\n",
        "          name='cnn_conv2d_kernel',\n",
        "          trainable=True),\n",
        "      #32个输出通道，每个输出通道都有一个对应的偏置\n",
        "      cnn_conv2d_bias=tf.Variable(\n",
        "          lambda: tf.zeros(dtype=tf.float32, shape=(32,)),\n",
        "          name='cnn_conv2d_bias',\n",
        "          trainable=True),\n",
        "      #第二层卷积,64个过滤器，共享权重矩阵为32*5*5,h_conv2.shape=[-1,14,14,64]\n",
        "      #第二个pooling层将其变成[-1,7,7,64]\n",
        "      cnn_conv2d_1_kernel=tf.Variable(\n",
        "          initial_value=kernel_initalizer(shape=(5,5,32,64), dtype=tf.float32),\n",
        "          name='cnn_conv2d_1_kernel',\n",
        "          trainable=True),\n",
        "      #64个输出通道对应64个偏置\n",
        "      cnn_conv2d_1_bias=tf.Variable(\n",
        "          lambda: tf.zeros(dtype=tf.float32, shape=(64,)),\n",
        "          name='cnn_conv2d_1_bias',\n",
        "          trainable=True),\n",
        "      #全连接层，现在图片尺寸减小到7×7，加入一个有1024个神经元的全连接层用于处理整个图片\n",
        "      #将池化层输出的张量reshape成一些7*7*64的向量，乘上权重加上偏置然后使用ReLU\n",
        "      cnn_dense_kernel=tf.Variable(\n",
        "          initial_value=kernel_initalizer(shape=(7*7*64,1024), dtype=tf.float32),\n",
        "          name='cnn_dense_kernel',\n",
        "          trainable=True),\n",
        "      cnn_dense_bias=tf.Variable(\n",
        "          lambda: tf.zeros(dtype=tf.float32, shape=(1024,)),\n",
        "          name='cnn_dense_bias',\n",
        "          trainable=True),\n",
        "      #这里以后可以加一个Dropout\n",
        "      cnn_dense_1_kernel=tf.Variable(\n",
        "          initial_value=kernel_initalizer(shape=(1024,10), dtype=tf.float32),\n",
        "          name='cnn_dense_1_kernel',\n",
        "          trainable=True),\n",
        "      cnn_dense_1_bias=tf.Variable(\n",
        "          lambda: tf.zeros(dtype=tf.float32, shape=(10,)),\n",
        "          name='cnn_dense_1_bias',\n",
        "          trainable=True),\n",
        "      num_examples=tf.Variable(0.0, name='num_examples', trainable=False),\n",
        "      loss_sum=tf.Variable(0.0, name='loss_sum', trainable=False),\n",
        "      accuracy_sum=tf.Variable(0.0, name='accuracy_sum', trainable=False)\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib6kQLwtUh5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#自定义前向传播函数\n",
        "def mnist_forward_pass(variables, batch):\n",
        "  _input_ = batch['x']  #取训练图片\n",
        "\n",
        "  #神经网络\n",
        "  #第一层卷积\n",
        "  conv = tf.nn.conv2d(_input_, variables.cnn_conv2d_kernel, strides=[1,1,1,1], padding='SAME')                  # [batch_size, 28, 28, 32]\n",
        "  pre_activation = tf.nn.bias_add(conv, variables.cnn_conv2d_bias)\n",
        "  conv1 = tf.nn.relu(pre_activation, name='conv1')\n",
        "\n",
        "  #第一层卷积层的池化\n",
        "  pool1 = tf.nn.max_pool2d(conv1, ksize=[2, 2], strides=2, padding='SAME')\n",
        "\n",
        "  #第二层卷积\n",
        "  conv = tf.nn.conv2d(pool1, variables.cnn_conv2d_1_kernel, strides=[1,1,1,1], padding='SAME')\n",
        "  pre_activation = tf.nn.bias_add(conv, variables.cnn_conv2d_1_bias)\n",
        "  conv2 = tf.nn.relu(pre_activation, name='conv2')\n",
        "\n",
        "  #第二层卷积的池化\n",
        "  pool2 = tf.nn.max_pool2d(conv2, ksize=[2,2], strides=2, padding='SAME')\n",
        "  print(\"pool2\",pool2)\n",
        "  \n",
        "  #flatten\n",
        "  flatten = tf.reshape(pool2, shape=(-1,7*7*64))\n",
        "  print(\"flatten over\")\n",
        "\n",
        "  #全连接层1\n",
        "  dense1 = tf.nn.relu(tf.matmul(flatten, variables.cnn_dense_kernel) + variables.cnn_dense_bias)\n",
        "  print(\"dense1 over\")\n",
        "\n",
        "  #添加一个Dropout防止过拟合\n",
        "  #keep_drop = tf.placeholder(tf.float32)\n",
        "  #drop_dense1 = tf.nn.tropout(dense1, variables.keep_drop)\n",
        "\n",
        "  #全连接层2\n",
        "  dense2 = tf.matmul(dense1, variables.cnn_dense_1_kernel) + variables.cnn_dense_1_bias\n",
        "  print(\"dense2 over\")\n",
        "\n",
        "  y_pred = tf.nn.softmax(dense2)\n",
        "  print(\"y_pred\",y_pred)\n",
        "\n",
        "  #计算损失\n",
        "  flat_labels = tf.reshape(batch['y'], [-1])\n",
        "  loss = -tf.reduce_mean(\n",
        "      tf.reduce_sum(tf.one_hot(flat_labels, 10) * tf.math.log(y_pred), axis=[1]))\n",
        "  \n",
        "  \n",
        "  #计算准确率\n",
        "  predictions = tf.cast(tf.argmax(y_pred,1), tf.int32)\n",
        "  accuracy = tf.reduce_mean(\n",
        "      tf.cast(tf.equal(predictions, flat_labels), tf.float32))\n",
        "  \n",
        "  #样本数\n",
        "  num_examples = tf.cast(tf.size(batch['y']), tf.float32)\n",
        "\n",
        "  #更新样本数、损失和、精度和,每一批都考虑了自己的权重\n",
        "  variables.num_examples.assign_add(num_examples)\n",
        "  variables.loss_sum.assign_add(loss * num_examples)\n",
        "  variables.accuracy_sum.assign_add(accuracy * num_examples)\n",
        "\n",
        "  \n",
        "  return loss, predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mcpE0twbxz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#计算本地用户的metrics度量\n",
        "def get_local_mnist_metrics(variables):\n",
        "  return collections.OrderedDict(\n",
        "      num_examples=variables.num_examples,\n",
        "      loss=variables.loss_sum / variables.num_examples,\n",
        "      accuracy=variables.accuracy_sum / variables.num_examples\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv_a5zrVch3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#集合每个设备发出的本地度量\n",
        "@tff.federated_computation\n",
        "def aggregate_mnist_metrics_across_clients(metrics):\n",
        "  return collections.OrderedDict(\n",
        "      num_examples=tff.federated_sum(metrics.num_examples),\n",
        "      loss=tff.federated_mean(metrics.loss, metrics.num_examples),\n",
        "      accuracy=tff.federated_mean(metrics.accuracy, metrics.num_examples)\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE3FpcO5dNr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#自定义模型，创建tff.learning.model实例\n",
        "class MnistModel(tff.learning.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._variables = create_mnist_variables()\n",
        "\n",
        "  #所有的“tf.Variables”都应该在“__init__”中引入\n",
        "  @property\n",
        "  def trainable_variables(self):\n",
        "    #return [self._variables.weights, self._variables.bias]\n",
        "    return [self._variables.cnn_conv2d_kernel,\n",
        "        self._variables.cnn_conv2d_bias,\n",
        "        self._variables.cnn_conv2d_1_kernel,\n",
        "        self._variables.cnn_conv2d_1_bias,\n",
        "        self._variables.cnn_dense_kernel,\n",
        "        self._variables.cnn_dense_bias,\n",
        "        self._variables.cnn_dense_1_kernel,\n",
        "        self._variables.cnn_dense_1_bias\n",
        "        ]\n",
        "  \n",
        "  @property\n",
        "  def non_trainable_variables(self):\n",
        "    return []\n",
        "  \n",
        "  @property\n",
        "  def local_variables(self):\n",
        "    return [self._variables.num_examples, self._variables.loss_sum,\n",
        "         self._variables.accuracy_sum]\n",
        "  \n",
        "  @property\n",
        "  def input_spec(self):\n",
        "    return collections.OrderedDict(\n",
        "        # x=tf.TensorSpec([None, 784], tf.float32),\n",
        "        x=tf.TensorSpec([None, 28,28,1], tf.float32),\n",
        "        y=tf.TensorSpec([None, 1], tf.int32)\n",
        "    )\n",
        "\n",
        "  @tf.function\n",
        "  def forward_pass(self, batch, training=True):\n",
        "    del training\n",
        "    loss, predictions = mnist_forward_pass(self._variables, batch)\n",
        "    #取出样本数\n",
        "    num_examples = tf.shape(batch['x'])[0]\n",
        "    return tff.learning.BatchOutput(\n",
        "        loss=loss, predictions=predictions, num_examples=num_examples)\n",
        "    \n",
        "  @tf.function\n",
        "  def report_local_outputs(self):\n",
        "    return get_local_mnist_metrics(self._variables)\n",
        "  \n",
        "  @property\n",
        "  def federated_output_computation(self):\n",
        "    return aggregate_mnist_metrics_across_clients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfqC79A4hVfw",
        "colab_type": "code",
        "outputId": "c7687c28-ebae-46d3-8792-cf9e5664d957",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "#创建迭代器执行联合平均的迭代过程\n",
        "iterative_process = tff.learning.build_federated_averaging_process(\n",
        "    MnistModel,\n",
        "    #client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(lr=0.001)\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pool2 Tensor(\"MaxPool2d_1:0\", shape=(None, 7, 7, 64), dtype=float32)\n",
            "flatten over\n",
            "dense1 over\n",
            "dense2 over\n",
            "y_pred Tensor(\"Softmax:0\", shape=(None, 10), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzJXeJNSlGpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#获得初始状态\n",
        "#state = iterative_process.initialize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpUydYwQnfHd",
        "colab_type": "code",
        "outputId": "faee72d6-ba11-4453-bd45-0280b55fc9ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "#查看第一轮训练\n",
        "sample_clients = random.sample(emnist_train.client_ids, NUM_CLIENTS)\n",
        "federated_train_data = make_federated_data(emnist_train, sample_clients)\n",
        "state, metrics = iterative_process.next(state, federated_train_data)\n",
        "print('round 1, metrics={}'.format(metrics))\n",
        "\"\"\""
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n#查看第一轮训练\\nsample_clients = random.sample(emnist_train.client_ids, NUM_CLIENTS)\\nfederated_train_data = make_federated_data(emnist_train, sample_clients)\\nstate, metrics = iterative_process.next(state, federated_train_data)\\nprint('round 1, metrics={}'.format(metrics))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkoGLYGLyYkC",
        "colab_type": "code",
        "outputId": "1f4ff961-d67a-4812-a40d-1bf2586c7718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "#获得初始状态\n",
        "state = iterative_process.initialize()\n",
        "#计算更多轮\n",
        "NUM_ROUNDS = 20\n",
        "for round_num in range(2,NUM_ROUNDS):\n",
        "  #随机选择NUM_CLIENTS个用户\n",
        "  sample_clients = random.sample(emnist_train.client_ids, NUM_CLIENTS)\n",
        "  #sample_clients=emnist_train.client_ids[3:6] # Debug\n",
        "  federated_train_data = make_federated_data(emnist_train, sample_clients)\n",
        "\n",
        "  state, metrics = iterative_process.next(state, federated_train_data)\n",
        "  print('round {:2d}, metrics={}'.format(round_num, metrics))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "round  2, metrics=<num_examples=1540.0,loss=2.2360072135925293,accuracy=0.2532467544078827>\n",
            "round  3, metrics=<num_examples=1540.0,loss=1.2697172164916992,accuracy=0.7422077655792236>\n",
            "round  4, metrics=<num_examples=1540.0,loss=0.4439338147640228,accuracy=0.8974025845527649>\n",
            "round  5, metrics=<num_examples=1540.0,loss=0.20304295420646667,accuracy=0.9493506550788879>\n",
            "round  6, metrics=<num_examples=1540.0,loss=0.10997801274061203,accuracy=0.9779220819473267>\n",
            "round  7, metrics=<num_examples=1540.0,loss=0.0502038337290287,accuracy=0.9896103739738464>\n",
            "round  8, metrics=<num_examples=1540.0,loss=0.03303653001785278,accuracy=0.9896103739738464>\n",
            "round  9, metrics=<num_examples=1540.0,loss=0.0171812791377306,accuracy=0.9941558241844177>\n",
            "round 10, metrics=<num_examples=1540.0,loss=0.010062076151371002,accuracy=0.9974026083946228>\n",
            "round 11, metrics=<num_examples=1540.0,loss=0.003629515413194895,accuracy=1.0>\n",
            "round 12, metrics=<num_examples=1540.0,loss=0.0054337456822395325,accuracy=0.9980519413948059>\n",
            "round 13, metrics=<num_examples=1540.0,loss=0.002862748922780156,accuracy=0.9993506669998169>\n",
            "round 14, metrics=<num_examples=1540.0,loss=0.01700676791369915,accuracy=0.9948052167892456>\n",
            "round 15, metrics=<num_examples=1540.0,loss=0.00609508715569973,accuracy=0.9974026083946228>\n",
            "round 16, metrics=<num_examples=1540.0,loss=0.0014321893686428666,accuracy=1.0>\n",
            "round 17, metrics=<num_examples=1540.0,loss=0.0008614161633886397,accuracy=1.0>\n",
            "round 18, metrics=<num_examples=1540.0,loss=0.0017379899509251118,accuracy=0.998701274394989>\n",
            "round 19, metrics=<num_examples=1540.0,loss=0.0006349303293973207,accuracy=1.0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKKb6c6h0AMF",
        "colab_type": "code",
        "outputId": "2fb13cb9-d880-45ec-d6b2-e2b1bb97b9c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "#使用tensorboard可视化\n",
        "#使用Tensorboard可视化这些联邦计算的度量\n",
        "#创建目录和相应的摘要编写器\n",
        "#@test {\"skip\": true}\n",
        "logdir = \"/tmp/logs/scalars/training/\"\n",
        "summary_writer = tf.summary.create_file_writer(logdir)\n",
        "state = iterative_process.initialize()\n",
        "\n",
        "#!!!太坑了，我以为@test是无关紧要的东西，emmm，就省略了，没想到就凉凉\n",
        "#@test {\"skip\": true}\n",
        "with summary_writer.as_default():\n",
        "  for round_num in range(1, NUM_ROUNDS):\n",
        "    #随机选择用户\n",
        "    sample_clients = random.sample(emnist_train.client_ids, NUM_CLIENTS)\n",
        "    federated_train_data = make_federated_data(emnist_train, sample_clients)\n",
        "\n",
        "    state, metrics = iterative_process.next(state, federated_train_data)\n",
        "\n",
        "    print('round {:2d}, metrics={}'.format(round_num, metrics))\n",
        "\n",
        "    for name, value in metrics._asdict().items():\n",
        "      tf.summary.scalar(name, value, step=round_num)\n",
        "\n",
        "#这个tensorBoard像有猫病一样，昨天还可以今天来突然就不行了\n",
        "#@test {\"skip\":true}\n",
        "#%load_ext tensorboard\n",
        "%tensorboard --logdir /tmp/logs/scalars/ --port=0\n",
        "'''"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#使用tensorboard可视化\\n#使用Tensorboard可视化这些联邦计算的度量\\n#创建目录和相应的摘要编写器\\n#@test {\"skip\": true}\\nlogdir = \"/tmp/logs/scalars/training/\"\\nsummary_writer = tf.summary.create_file_writer(logdir)\\nstate = iterative_process.initialize()\\n\\n#!!!太坑了，我以为@test是无关紧要的东西，emmm，就省略了，没想到就凉凉\\n#@test {\"skip\": true}\\nwith summary_writer.as_default():\\n  for round_num in range(1, NUM_ROUNDS):\\n    #随机选择用户\\n    sample_clients = random.sample(emnist_train.client_ids, NUM_CLIENTS)\\n    federated_train_data = make_federated_data(emnist_train, sample_clients)\\n\\n    state, metrics = iterative_process.next(state, federated_train_data)\\n\\n    print(\\'round {:2d}, metrics={}\\'.format(round_num, metrics))\\n\\n    for name, value in metrics._asdict().items():\\n      tf.summary.scalar(name, value, step=round_num)\\n\\n#这个tensorBoard像有猫病一样，昨天还可以今天来突然就不行了\\n#@test {\"skip\":true}\\n#%load_ext tensorboard\\n%tensorboard --logdir /tmp/logs/scalars/ --port=0\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBGzrtDgP8hn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#在测试集上进行测试\n",
        "#求值传递MnistModel就足够了，不执行梯度下降，也不需要构造优化器，MnistTModel就是前面定义的那个class\n",
        "evaluation = tff.learning.build_federated_evaluation(MnistModel)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rH41dJGqKDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#评估一下在训练中得到的最新状态，为了从服务器状态中提取最新的训练模型，只需访问.model成员\n",
        "#和上面说的形式一样，它接收model和联邦数据并返回训练的metrics\n",
        "#这里是在评估模型的最新状态，为了从服务器状态中提取最新的训练模型，只需访问.model成员\n",
        "train_metrics = evaluation(state.model, federated_train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sT7Lwd1qNxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#编译联邦数据的测试样本，并对测试数据重新运行求值。同样也是随机选择NUM_CLIENTS个用户。\n",
        "sample_clients = random.sample(emnist_train.client_ids, NUM_CLIENTS)\n",
        "federated_test_data = make_federated_data(emnist_test, sample_clients)\n",
        "len(federated_test_data), federated_test_data[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhYHpLYsqT_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_metrics = evaluation(state.model, federated_test_data)\n",
        "#初始精度在0.8898\n",
        "str(test_metrics)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}