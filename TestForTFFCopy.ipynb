{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestForTFFCopy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOlbLW4qhFlcRjRo5KZtN6B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Next-Sunshine/TFF0318/blob/master/TestForTFFCopy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9pTTi0lPh1w",
        "colab_type": "code",
        "outputId": "96462c19-06b9-45c0-b087-39df7059ff01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "#环境测试\n",
        "#@test {\"skip\":true}\n",
        "!pip install --quiet --upgrade tensorflow_federated\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 430kB 7.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8MB 23.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 41.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 421.8MB 42kB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 38.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 13.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 20.0MB 161kB/s \n",
            "\u001b[K     |████████████████████████████████| 450kB 42.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.9MB 31.0MB/s \n",
            "\u001b[?25h  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITkxSUMiRa01",
        "colab_type": "code",
        "outputId": "93615fcb-ff35-4f68-f2b9-faf28a85f29c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import h5py\n",
        "\n",
        "tf.compat.v1.enable_v2_behavior()\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "tff.federated_computation(lambda: 'Hello, world!')()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'Hello, world!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRJgOTiAR3T4",
        "colab_type": "code",
        "outputId": "f9cdca5d-2b48-4c1a-a690-97c422f583b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "#装载数据集，实验使用EMNIST数据集\n",
        "emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()\n",
        "#装载mnist数据集\n",
        "mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tff-datasets-public/fed_emnist_digitsonly.tar.bz2\n",
            "97402880/97398400 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p1XDcJPi_M-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#从mnist_train中读60000张图片放入一个数组中以备用\n",
        "class_ = [[]for i in range(10)]\n",
        "for i in range(len(mnist_train[1])):\n",
        "  label = mnist_train[1][i]\n",
        "  class_[label].append(mnist_train[0][i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a39F3CRqX9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#当创建数据集需要修改的时候就要用这里，先清空再重新创建\n",
        "f.flush()\n",
        "f.clear()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz2h0TU23dsp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#设置各个不平衡的数据集，首先弄一个BAL1:10个类别，每个类别数目相等，每个用户都含有10个类别且每类数目相等，均12个\n",
        "#创建自己的数据集首先是BAL1：全平衡，使用了mnist数据集来分配，60000张图片平均分给500个客户端，每个客户端分得10类。每类120张图片\n",
        "f = h5py.File(\"BAL1.hdf5\",\"a\")\n",
        "top_group = f.create_group(\"examples\")   #创建顶层group名为examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkNHRXhikc8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MY_NUM_CLIENTS = 20  #写的时候最初用50，用500时间太长，试过500，磁盘会从30G飙升到60G且还运行不完，不知道后面500个客户端的时候怎么办\n",
        "NUM_PER_CLASS = 12  #每个客户端每类的图片数量\n",
        "#Collections.Orderdict->TensorSliceDataset+client_id->hdf5->HDF5ClientData\n",
        "#<TensorSliceDataset shapes: OrderedDict([(label, ()), (pixels, (28, 28))]), types: OrderedDict([(label, tf.int32), (pixels, tf.float32)])>\n",
        "pixels = []\n",
        "label=[]\n",
        "client_ids = []\n",
        "for i in range(MY_NUM_CLIENTS):  #i客户端数量\n",
        "  for j in range(10):  #j是类别数\n",
        "    for k in range(NUM_PER_CLASS): #k控制每一类图片放多少张在客户端\n",
        "      pixels.append(class_[j][i*NUM_PER_CLASS+k]/255.0) #转成0-1之内的数字再加入pixels，class_[j]存放的是第j类数\n",
        "      label.append(j)  #加入label\n",
        "  \n",
        "  client_ids.append(\"f00_\" + str(i))\n",
        "  temp_group = top_group.create_group(name=\"f00_\" + str(i))  #创建以client_id为名字的group\n",
        "  label_ds = temp_group.create_dataset(name=\"label\", data=np.array(label, dtype='int32'))  #向group中写入label\n",
        "  pixels_ds = temp_group.create_dataset(name=\"pixels\", data=np.array(pixels, dtype='float32'))  #向group中写入pixels\n",
        "\n",
        "  #清空上一个pixels数组和label数组中存放的上一个客户端的数据信息,否则会有灾难！！！\n",
        "  pixels.clear()\n",
        "  label.clear()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLcRrX6pk41S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#实例化构造的数据集\n",
        "BAL1 = tff.simulation.hdf5_client_data.HDF5ClientData(\"BAL1.hdf5\")\n",
        "num_clients_BAL1 = len(BAL1.client_ids)\n",
        "#留出总客户端的4分之一做测试集\n",
        "BAL1_train, BAL1_test = BAL1.train_test_client_split(client_data=BAL1, num_test_clients= num_clients_BAL1//4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKth4b_uidw3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cd3981e0-dfaf-49af-adde-3d5c7669e7bf"
      },
      "source": [
        "print(len(BAL1_train.client_ids))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPD6BU2unLwo",
        "colab_type": "code",
        "outputId": "b96fe252-6210-40c6-f746-0ad60b9c7035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#BAL1.client_ids\n",
        "#这里面的每一个打印出来都是和example_dataset类型一样的\n",
        "\n",
        "x = tf.data.Dataset.from_tensor_slices(\n",
        "        collections.OrderedDict((name, ds.value) for name, ds in sorted(\n",
        "            f[\"examples\"][BAL1.client_ids[0]].items())))\n",
        "print(x)\n",
        "example_dataset = emnist_train.create_tf_dataset_for_client(\n",
        "    emnist_train.client_ids[0]\n",
        ")\n",
        "print(example_dataset)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<TensorSliceDataset shapes: OrderedDict([(label, ()), (pixels, (28, 28))]), types: OrderedDict([(label, tf.int32), (pixels, tf.float32)])>\n",
            "<TensorSliceDataset shapes: OrderedDict([(label, ()), (pixels, (28, 28))]), types: OrderedDict([(label, tf.int32), (pixels, tf.float32)])>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHXBNT_UxIWo",
        "colab_type": "code",
        "outputId": "052bf2f4-a609-46d8-8dd6-63f2f95c70a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#此处得到客户端的总数K，因为每一轮随机选择C×K的向上取整\n",
        "#根据FedAvg做的研究C取0.2可获得较好的性能，为了简化模型K这里取固定的数\n",
        "K = len(emnist_train.client_ids)\n",
        "C = 0.2\n",
        "K"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3383"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip--KI1UhiSU",
        "colab_type": "text"
      },
      "source": [
        "以上是产生数据集的部分，以下是预处理以及其他部分"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc578aa2UuZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NUM_CLIENTS在实验的时候应该是K×C\n",
        "NUM_CLIENTS = 10\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 20\n",
        "SHUFFLE_BUFFER = 100\n",
        "PREFETCH_BUFFER = 10\n",
        "\n",
        "#预处理，将图片中的‘pixels'、‘label'分别表示成x和y\n",
        "#将28×28的图像展平成784个元素，打乱顺序\n",
        "def preprocess(dataset):\n",
        "  #内部函数将像素和标签转换成x和y，并将像素展平\n",
        "  def batch_format_fn(element):\n",
        "    return collections.OrderedDict(\n",
        "        #因为是灰度图所以通道数为1，如果是rgb图片则为3\n",
        "        #因为CNN卷积层接收一个4d向量\n",
        "        x = tf.reshape(element['pixels'], [-1,28,28,1]),\n",
        "        y = tf.reshape(element['label'], [-1,1])\n",
        "    )\n",
        "  \n",
        "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRqmbbzamP1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#为指定用户创建联邦数据,接收训练集和用户id\n",
        "def make_federated_data(client_data, client_ids):\n",
        "  return [\n",
        "      preprocess(client_data.create_tf_dataset_for_client(x))\n",
        "      for x in client_ids\n",
        "    ]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDHJSC8OYaZj",
        "colab_type": "code",
        "outputId": "635cec2e-a2c1-4177-dd83-70b0e4b5bab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#这里改成随机获得NUM_CLIENTS个用户，模拟联邦学习每轮的随机选择用户\n",
        "# sample_clients = random.sample(emnist_train.client_ids, NUM_CLIENTS)\n",
        "# federated_train_data = make_federated_data(emnist_train, sample_clients)\n",
        "example_dataset = BAL1.create_tf_dataset_for_client(\n",
        "    BAL1.client_ids[0]\n",
        ")\n",
        "print(\"examle_dataset\",example_dataset)\n",
        "# sample_clients = emnist_train.client_ids[0:3]\n",
        "# federated_train_data = make_federated_data(emnist_train, sample_clients)\n",
        "\n",
        "#BAL1数据集测试\n",
        "#sample_clients = random.sample(BAL1.client_ids, NUM_CLIENTS)\n",
        "sample_clients = BAL1.client_ids[0:NUM_CLIENTS]\n",
        "federated_train_data = make_federated_data(BAL1, sample_clients)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "examle_dataset <TensorSliceDataset shapes: OrderedDict([(label, ()), (pixels, (28, 28))]), types: OrderedDict([(label, tf.int32), (pixels, tf.float32)])>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZEcia5FltoP",
        "colab_type": "text"
      },
      "source": [
        "间隔界"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4MRt7FEh2gq",
        "colab_type": "code",
        "outputId": "42e40599-4bde-4d11-b996-3adeb644a42f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "'''\n",
        "#！自己添加代码中，正在想办法把训练模型改成CNN\n",
        "class CNN(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(\n",
        "            filters=32,             # 卷积层神经元（卷积核）数目\n",
        "            kernel_size=[5, 5],     # 感受野大小\n",
        "            padding='same',         # padding策略（vaild 或 same）\n",
        "            activation=tf.nn.relu   # 激活函数\n",
        "        )\n",
        "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
        "        self.conv2 = tf.keras.layers.Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=[5, 5],\n",
        "            padding='same',\n",
        "            activation=tf.nn.relu\n",
        "        )\n",
        "        #print(\"kernel:\",kernel_size)\n",
        "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
        "        self.flatten = tf.keras.layers.Reshape(target_shape=(7 * 7 * 64,))\n",
        "        self.dense1 = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)\n",
        "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
        " \n",
        "    def call(self, inputs):\n",
        "        x = self.conv1(inputs)                  # [batch_size, 28, 28, 32]\n",
        "        x = self.pool1(x)                       # [batch_size, 14, 14, 32]\n",
        "        x = self.conv2(x)                       # [batch_size, 14, 14, 64]\n",
        "        x = self.pool2(x)                       # [batch_size, 7, 7, 64]\n",
        "        x = self.flatten(x)                     # [batch_size, 7 * 7 * 64]\n",
        "        x = self.dense1(x)                      # [batch_size, 1024]\n",
        "        x = self.dense2(x)\n",
        "        #tf.nn.softmax()将原始输出归一化，且能凸显原始向量中最大的值                      # [batch_size, 10]\n",
        "        output = tf.nn.softmax(x)\n",
        "        return output\n",
        "        '''\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#！自己添加代码中，正在想办法把训练模型改成CNN\\nclass CNN(tf.keras.Model):\\n    def __init__(self):\\n        super().__init__()\\n        self.conv1 = tf.keras.layers.Conv2D(\\n            filters=32,             # 卷积层神经元（卷积核）数目\\n            kernel_size=[5, 5],     # 感受野大小\\n            padding=\\'same\\',         # padding策略（vaild 或 same）\\n            activation=tf.nn.relu   # 激活函数\\n        )\\n        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\\n        self.conv2 = tf.keras.layers.Conv2D(\\n            filters=64,\\n            kernel_size=[5, 5],\\n            padding=\\'same\\',\\n            activation=tf.nn.relu\\n        )\\n        #print(\"kernel:\",kernel_size)\\n        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\\n        self.flatten = tf.keras.layers.Reshape(target_shape=(7 * 7 * 64,))\\n        self.dense1 = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)\\n        self.dense2 = tf.keras.layers.Dense(units=10)\\n \\n    def call(self, inputs):\\n        x = self.conv1(inputs)                  # [batch_size, 28, 28, 32]\\n        x = self.pool1(x)                       # [batch_size, 14, 14, 32]\\n        x = self.conv2(x)                       # [batch_size, 14, 14, 64]\\n        x = self.pool2(x)                       # [batch_size, 7, 7, 64]\\n        x = self.flatten(x)                     # [batch_size, 7 * 7 * 64]\\n        x = self.dense1(x)                      # [batch_size, 1024]\\n        x = self.dense2(x)\\n        #tf.nn.softmax()将原始输出归一化，且能凸显原始向量中最大的值                      # [batch_size, 10]\\n        output = tf.nn.softmax(x)\\n        return output\\n        '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw419GpLlmz9",
        "colab_type": "text"
      },
      "source": [
        "间隔界\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNo6O_5qSNSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#创建一个变量集合来表示所有变量,包括model(weights+bias)以及metrics(num_examples,loss_sum,accuracy_sum)\n",
        "MnistVariables = collections.namedtuple(\n",
        "    'MnistVariables','cnn_conv2d_kernel cnn_conv2d_bias cnn_conv2d_1_kernel cnn_conv2d_1_bias cnn_dense_kernel cnn_dense_bias cnn_dense_1_kernel cnn_dense_1_bias num_examples loss_sum accuracy_sum'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSEDLYxJTZ8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#创建变量并初始化\n",
        "def create_mnist_variables():\n",
        "  #核初始化函数，使用正态分布，先使用tf.XXX(初始参数)生成一个模型关键字C，再使用C(arg1)传参数进去\n",
        "  kernel_initalizer=tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05)\n",
        "  return MnistVariables(\n",
        "      #第一层卷积在5×5的patch中算出32个特征，权重张量形状为[5,5,1,32]\n",
        "      #前两个维度是patch的大小，接着是输入的通道数，最后是输出的通道数目\n",
        "      #h_conv1.shape=[-1,28,28,32]，第一个pooling层将其变成[-1,14,14,32]     \n",
        "      cnn_conv2d_kernel=tf.Variable(\n",
        "          #权重初始化至关重要，不能全部初始化为0！！！\n",
        "          initial_value=kernel_initalizer(shape=(5,5,1,32), dtype=tf.float32),\n",
        "          name='cnn_conv2d_kernel',\n",
        "          trainable=True),\n",
        "      #32个输出通道，每个输出通道都有一个对应的偏置\n",
        "      cnn_conv2d_bias=tf.Variable(\n",
        "          lambda: tf.zeros(dtype=tf.float32, shape=(32,)),\n",
        "          name='cnn_conv2d_bias',\n",
        "          trainable=True),\n",
        "      #第二层卷积,64个过滤器，共享权重矩阵为32*5*5,h_conv2.shape=[-1,14,14,64]\n",
        "      #第二个pooling层将其变成[-1,7,7,64]\n",
        "      cnn_conv2d_1_kernel=tf.Variable(\n",
        "          initial_value=kernel_initalizer(shape=(5,5,32,64), dtype=tf.float32),\n",
        "          name='cnn_conv2d_1_kernel',\n",
        "          trainable=True),\n",
        "      #64个输出通道对应64个偏置\n",
        "      cnn_conv2d_1_bias=tf.Variable(\n",
        "          lambda: tf.zeros(dtype=tf.float32, shape=(64,)),\n",
        "          name='cnn_conv2d_1_bias',\n",
        "          trainable=True),\n",
        "      #全连接层，现在图片尺寸减小到7×7，加入一个有1024个神经元的全连接层用于处理整个图片\n",
        "      #将池化层输出的张量reshape成一些7*7*64的向量，乘上权重加上偏置然后使用ReLU\n",
        "      cnn_dense_kernel=tf.Variable(\n",
        "          initial_value=kernel_initalizer(shape=(7*7*64,1024), dtype=tf.float32),\n",
        "          name='cnn_dense_kernel',\n",
        "          trainable=True),\n",
        "      cnn_dense_bias=tf.Variable(\n",
        "          lambda: tf.zeros(dtype=tf.float32, shape=(1024,)),\n",
        "          name='cnn_dense_bias',\n",
        "          trainable=True),\n",
        "      #这里以后可以加一个Dropout\n",
        "      cnn_dense_1_kernel=tf.Variable(\n",
        "          initial_value=kernel_initalizer(shape=(1024,10), dtype=tf.float32),\n",
        "          name='cnn_dense_1_kernel',\n",
        "          trainable=True),\n",
        "      cnn_dense_1_bias=tf.Variable(\n",
        "          lambda: tf.zeros(dtype=tf.float32, shape=(10,)),\n",
        "          name='cnn_dense_1_bias',\n",
        "          trainable=True),\n",
        "      num_examples=tf.Variable(0.0, name='num_examples', trainable=False),\n",
        "      loss_sum=tf.Variable(0.0, name='loss_sum', trainable=False),\n",
        "      accuracy_sum=tf.Variable(0.0, name='accuracy_sum', trainable=False)\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib6kQLwtUh5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#自定义前向传播函数\n",
        "def mnist_forward_pass(variables, batch):\n",
        "  _input_ = batch['x']  #取训练图片\n",
        "\n",
        "  #神经网络\n",
        "  #第一层卷积\n",
        "  conv = tf.nn.conv2d(_input_, variables.cnn_conv2d_kernel, strides=[1,1,1,1], padding='SAME')                  # [batch_size, 28, 28, 32]\n",
        "  pre_activation = tf.nn.bias_add(conv, variables.cnn_conv2d_bias)\n",
        "  conv1 = tf.nn.relu(pre_activation, name='conv1')\n",
        "\n",
        "  #第一层卷积层的池化\n",
        "  pool1 = tf.nn.max_pool2d(conv1, ksize=[2, 2], strides=2, padding='SAME')\n",
        "\n",
        "  #第二层卷积\n",
        "  conv = tf.nn.conv2d(pool1, variables.cnn_conv2d_1_kernel, strides=[1,1,1,1], padding='SAME')\n",
        "  pre_activation = tf.nn.bias_add(conv, variables.cnn_conv2d_1_bias)\n",
        "  conv2 = tf.nn.relu(pre_activation, name='conv2')\n",
        "\n",
        "  #第二层卷积的池化\n",
        "  pool2 = tf.nn.max_pool2d(conv2, ksize=[2,2], strides=2, padding='SAME')\n",
        "  print(\"pool2\",pool2)\n",
        "  \n",
        "  #flatten\n",
        "  flatten = tf.reshape(pool2, shape=(-1,7*7*64))\n",
        "  print(\"flatten over\")\n",
        "\n",
        "  #全连接层1\n",
        "  dense1 = tf.nn.relu(tf.matmul(flatten, variables.cnn_dense_kernel) + variables.cnn_dense_bias)\n",
        "  print(\"dense1 over\")\n",
        "\n",
        "  #添加一个Dropout防止过拟合\n",
        "  #keep_drop = tf.placeholder(tf.float32)\n",
        "  #drop_dense1 = tf.nn.tropout(dense1, variables.keep_drop)\n",
        "\n",
        "  #全连接层2\n",
        "  dense2 = tf.matmul(dense1, variables.cnn_dense_1_kernel) + variables.cnn_dense_1_bias\n",
        "  print(\"dense2 over\")\n",
        "\n",
        "  y_pred = tf.nn.softmax(dense2)\n",
        "  print(\"y_pred\",y_pred)\n",
        "\n",
        "  #计算损失\n",
        "  flat_labels = tf.reshape(batch['y'], [-1])\n",
        "  loss = -tf.reduce_mean(\n",
        "      tf.reduce_sum(tf.one_hot(flat_labels, 10) * tf.math.log(y_pred), axis=[1]))\n",
        "  \n",
        "  \n",
        "  #计算准确率\n",
        "  predictions = tf.cast(tf.argmax(y_pred,1), tf.int32)\n",
        "  accuracy = tf.reduce_mean(\n",
        "      tf.cast(tf.equal(predictions, flat_labels), tf.float32))\n",
        "  \n",
        "  #样本数\n",
        "  num_examples = tf.cast(tf.size(batch['y']), tf.float32)\n",
        "\n",
        "  #更新样本数、损失和、精度和,每一批都考虑了自己的权重\n",
        "  variables.num_examples.assign_add(num_examples)\n",
        "  variables.loss_sum.assign_add(loss * num_examples)\n",
        "  variables.accuracy_sum.assign_add(accuracy * num_examples)\n",
        "\n",
        "  \n",
        "  return loss, predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mcpE0twbxz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#计算本地用户的metrics度量\n",
        "def get_local_mnist_metrics(variables):\n",
        "  return collections.OrderedDict(\n",
        "      num_examples=variables.num_examples,\n",
        "      loss=variables.loss_sum / variables.num_examples,\n",
        "      accuracy=variables.accuracy_sum / variables.num_examples\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv_a5zrVch3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#集合每个设备发出的本地度量\n",
        "@tff.federated_computation\n",
        "def aggregate_mnist_metrics_across_clients(metrics):\n",
        "  return collections.OrderedDict(\n",
        "      num_examples=tff.federated_sum(metrics.num_examples),\n",
        "      loss=tff.federated_mean(metrics.loss, metrics.num_examples),\n",
        "      accuracy=tff.federated_mean(metrics.accuracy, metrics.num_examples)\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE3FpcO5dNr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#自定义模型，创建tff.learning.model实例\n",
        "class MnistModel(tff.learning.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._variables = create_mnist_variables()\n",
        "\n",
        "  #所有的“tf.Variables”都应该在“__init__”中引入\n",
        "  @property\n",
        "  def trainable_variables(self):\n",
        "    #return [self._variables.weights, self._variables.bias]\n",
        "    return [self._variables.cnn_conv2d_kernel,\n",
        "        self._variables.cnn_conv2d_bias,\n",
        "        self._variables.cnn_conv2d_1_kernel,\n",
        "        self._variables.cnn_conv2d_1_bias,\n",
        "        self._variables.cnn_dense_kernel,\n",
        "        self._variables.cnn_dense_bias,\n",
        "        self._variables.cnn_dense_1_kernel,\n",
        "        self._variables.cnn_dense_1_bias\n",
        "        ]\n",
        "  \n",
        "  @property\n",
        "  def non_trainable_variables(self):\n",
        "    return []\n",
        "  \n",
        "  @property\n",
        "  def local_variables(self):\n",
        "    return [self._variables.num_examples, self._variables.loss_sum,\n",
        "         self._variables.accuracy_sum]\n",
        "  \n",
        "  @property\n",
        "  def input_spec(self):\n",
        "    return collections.OrderedDict(\n",
        "        x=tf.TensorSpec([None, 28,28,1], tf.float32),\n",
        "        y=tf.TensorSpec([None, 1], tf.int32)\n",
        "    )\n",
        "\n",
        "  @tf.function\n",
        "  def forward_pass(self, batch, training=True):\n",
        "    del training\n",
        "    loss, predictions = mnist_forward_pass(self._variables, batch)\n",
        "    #取出样本数\n",
        "    num_examples = tf.shape(batch['x'])[0]\n",
        "    return tff.learning.BatchOutput(\n",
        "        loss=loss, predictions=predictions, num_examples=num_examples)\n",
        "    \n",
        "  @tf.function\n",
        "  def report_local_outputs(self):\n",
        "    return get_local_mnist_metrics(self._variables)\n",
        "  \n",
        "  @property\n",
        "  def federated_output_computation(self):\n",
        "    return aggregate_mnist_metrics_across_clients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfqC79A4hVfw",
        "colab_type": "code",
        "outputId": "63502e58-8d4b-41e7-a618-590fd15e4d1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "#创建迭代器执行联合平均的迭代过程\n",
        "iterative_process = tff.learning.build_federated_averaging_process(\n",
        "    MnistModel,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02)\n",
        ")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pool2 Tensor(\"MaxPool2d_1:0\", shape=(None, 7, 7, 64), dtype=float32)\n",
            "flatten over\n",
            "dense1 over\n",
            "dense2 over\n",
            "y_pred Tensor(\"Softmax:0\", shape=(None, 10), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzJXeJNSlGpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#获得初始状态\n",
        "state = iterative_process.initialize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpUydYwQnfHd",
        "colab_type": "code",
        "outputId": "7bd1a815-54b3-45b5-dc91-d894545189f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#查看第一轮训练\n",
        "# sample_clients = random.sample(emnist_train.client_ids, NUM_CLIENTS)\n",
        "# federated_train_data = make_federated_data(emnist_train, sample_clients)\n",
        "#BAL1数据集测试\n",
        "# sample_clients = random.sample(BAL1.client_ids, NUM_CLIENTS)\n",
        "# federated_train_data = make_federated_data(BAL1, sample_clients)\n",
        "\n",
        "sample_clients = BAL1.client_ids[0:NUM_CLIENTS]\n",
        "federated_train_data = make_federated_data(BAL1, sample_clients)\n",
        "\n",
        "state, metrics = iterative_process.next(state, federated_train_data)\n",
        "print('round 1, metrics={}'.format(metrics))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "round 1, metrics=<num_examples=6000.0,loss=2.066589593887329,accuracy=0.39649999141693115>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkoGLYGLyYkC",
        "colab_type": "code",
        "outputId": "d9783910-f478-45db-8774-136b2d9d81f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "source": [
        "#计算更多轮\n",
        "NUM_ROUNDS = 30\n",
        "for round_num in range(2,NUM_ROUNDS):\n",
        "  #随机选择NUM_CLIENTS个用户\n",
        "  # sample_clients = random.sample(emnist_train.client_ids, NUM_CLIENTS)\n",
        "  # federated_train_data = make_federated_data(emnist_train, sample_clients)\n",
        "\n",
        "  sample_clients = random.sample(BAL1_train.client_ids, NUM_CLIENTS)\n",
        "  federated_train_data = make_federated_data(BAL1_train, sample_clients)\n",
        "\n",
        "  state, metrics = iterative_process.next(state, federated_train_data)\n",
        "  print('round {:2d}, metrics={}'.format(round_num, metrics))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "round  2, metrics=<num_examples=6000.0,loss=2.0423457622528076,accuracy=0.38499999046325684>\n",
            "round  3, metrics=<num_examples=6000.0,loss=1.2473881244659424,accuracy=0.643833339214325>\n",
            "round  4, metrics=<num_examples=6000.0,loss=0.6714187860488892,accuracy=0.8021666407585144>\n",
            "round  5, metrics=<num_examples=6000.0,loss=0.4409526288509369,accuracy=0.8756666779518127>\n",
            "round  6, metrics=<num_examples=6000.0,loss=0.3223763108253479,accuracy=0.9086666703224182>\n",
            "round  7, metrics=<num_examples=6000.0,loss=0.24849900603294373,accuracy=0.9356666803359985>\n",
            "round  8, metrics=<num_examples=6000.0,loss=0.2506083548069,accuracy=0.9315000176429749>\n",
            "round  9, metrics=<num_examples=6000.0,loss=0.17909502983093262,accuracy=0.9526666402816772>\n",
            "round 10, metrics=<num_examples=6000.0,loss=0.1639392226934433,accuracy=0.9585000276565552>\n",
            "round 11, metrics=<num_examples=6000.0,loss=0.17241646349430084,accuracy=0.953000009059906>\n",
            "round 12, metrics=<num_examples=6000.0,loss=0.14724187552928925,accuracy=0.9610000252723694>\n",
            "round 13, metrics=<num_examples=6000.0,loss=0.13204814493656158,accuracy=0.9668333530426025>\n",
            "round 14, metrics=<num_examples=6000.0,loss=0.12641453742980957,accuracy=0.9670000076293945>\n",
            "round 15, metrics=<num_examples=6000.0,loss=0.10605562478303909,accuracy=0.9738333225250244>\n",
            "round 16, metrics=<num_examples=6000.0,loss=0.09247854351997375,accuracy=0.9773333072662354>\n",
            "round 17, metrics=<num_examples=6000.0,loss=0.0986480712890625,accuracy=0.9756666421890259>\n",
            "round 18, metrics=<num_examples=6000.0,loss=0.08352109789848328,accuracy=0.9803333282470703>\n",
            "round 19, metrics=<num_examples=6000.0,loss=0.07391303777694702,accuracy=0.9810000061988831>\n",
            "round 20, metrics=<num_examples=6000.0,loss=0.08867672830820084,accuracy=0.9778333306312561>\n",
            "round 21, metrics=<num_examples=6000.0,loss=0.07087718695402145,accuracy=0.9819999933242798>\n",
            "round 22, metrics=<num_examples=6000.0,loss=0.05854591354727745,accuracy=0.9860000014305115>\n",
            "round 23, metrics=<num_examples=6000.0,loss=0.052744701504707336,accuracy=0.9883333444595337>\n",
            "round 24, metrics=<num_examples=6000.0,loss=0.0600084587931633,accuracy=0.9861666560173035>\n",
            "round 25, metrics=<num_examples=6000.0,loss=0.0449615903198719,accuracy=0.9908333420753479>\n",
            "round 26, metrics=<num_examples=6000.0,loss=0.0460515134036541,accuracy=0.9893333315849304>\n",
            "round 27, metrics=<num_examples=6000.0,loss=0.045863356441259384,accuracy=0.9898333549499512>\n",
            "round 28, metrics=<num_examples=6000.0,loss=0.04376857355237007,accuracy=0.9901666641235352>\n",
            "round 29, metrics=<num_examples=6000.0,loss=0.06524310261011124,accuracy=0.9865000247955322>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKKb6c6h0AMF",
        "colab_type": "code",
        "outputId": "785c816b-c891-43e0-86ba-1fa76fabacc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        }
      },
      "source": [
        "'''\n",
        "#使用tensorboard可视化\n",
        "#使用Tensorboard可视化这些联邦计算的度量\n",
        "#创建目录和相应的摘要编写器\n",
        "#@test {\"skip\": true}\n",
        "logdir = \"/tmp/logs/scalars/training/\"\n",
        "summary_writer = tf.summary.create_file_writer(logdir)\n",
        "state = iterative_process.initialize()\n",
        "\n",
        "#!!!太坑了，我以为@test是无关紧要的东西，emmm，就省略了，没想到就凉凉\n",
        "#@test {\"skip\": true}\n",
        "with summary_writer.as_default():\n",
        "  for round_num in range(1, NUM_ROUNDS):\n",
        "    #随机选择用户\n",
        "    sample_clients = random.sample(emnist_train.client_ids, NUM_CLIENTS)\n",
        "    federated_train_data = make_federated_data(emnist_train, sample_clients)\n",
        "\n",
        "    state, metrics = iterative_process.next(state, federated_train_data)\n",
        "\n",
        "    print('round {:2d}, metrics={}'.format(round_num, metrics))\n",
        "\n",
        "    for name, value in metrics._asdict().items():\n",
        "      tf.summary.scalar(name, value, step=round_num)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "round  1, metrics=<num_examples=4520.0,loss=2.2766871452331543,accuracy=0.1285398304462433>\n",
            "round  2, metrics=<num_examples=4755.0,loss=2.258615493774414,accuracy=0.18107254803180695>\n",
            "round  3, metrics=<num_examples=4835.0,loss=2.224494218826294,accuracy=0.242192342877388>\n",
            "round  4, metrics=<num_examples=5030.0,loss=2.1682469844818115,accuracy=0.3049701750278473>\n",
            "round  5, metrics=<num_examples=5260.0,loss=2.093106508255005,accuracy=0.3969581723213196>\n",
            "round  6, metrics=<num_examples=4775.0,loss=1.9683749675750732,accuracy=0.43287956714630127>\n",
            "round  7, metrics=<num_examples=4890.0,loss=1.7429050207138062,accuracy=0.5300613641738892>\n",
            "round  8, metrics=<num_examples=4750.0,loss=1.5358659029006958,accuracy=0.5734736919403076>\n",
            "round  9, metrics=<num_examples=5310.0,loss=1.1962789297103882,accuracy=0.6866289973258972>\n",
            "round 10, metrics=<num_examples=5055.0,loss=0.94955974817276,accuracy=0.7507418394088745>\n",
            "round 11, metrics=<num_examples=5035.0,loss=0.7275433540344238,accuracy=0.8077457547187805>\n",
            "round 12, metrics=<num_examples=5620.0,loss=0.5517209768295288,accuracy=0.8720640540122986>\n",
            "round 13, metrics=<num_examples=4890.0,loss=0.4399041533470154,accuracy=0.8867075443267822>\n",
            "round 14, metrics=<num_examples=5310.0,loss=0.5845663547515869,accuracy=0.8380414247512817>\n",
            "round 15, metrics=<num_examples=4785.0,loss=0.3490145802497864,accuracy=0.9161964654922485>\n",
            "round 16, metrics=<num_examples=4915.0,loss=0.3346066176891327,accuracy=0.9123092293739319>\n",
            "round 17, metrics=<num_examples=5115.0,loss=0.49228382110595703,accuracy=0.8469208478927612>\n",
            "round 18, metrics=<num_examples=4730.0,loss=0.2691667675971985,accuracy=0.9308667778968811>\n",
            "round 19, metrics=<num_examples=4920.0,loss=0.32418715953826904,accuracy=0.9148374199867249>\n",
            "round 20, metrics=<num_examples=5105.0,loss=0.33962374925613403,accuracy=0.9085210561752319>\n",
            "round 21, metrics=<num_examples=5415.0,loss=0.2604932188987732,accuracy=0.9337027072906494>\n",
            "round 22, metrics=<num_examples=5255.0,loss=0.20772168040275574,accuracy=0.9448144435882568>\n",
            "round 23, metrics=<num_examples=5255.0,loss=0.14530666172504425,accuracy=0.9598477482795715>\n",
            "round 24, metrics=<num_examples=5085.0,loss=0.14056502282619476,accuracy=0.9649950861930847>\n",
            "round 25, metrics=<num_examples=4960.0,loss=0.3246842324733734,accuracy=0.9106854796409607>\n",
            "round 26, metrics=<num_examples=5155.0,loss=0.16446436941623688,accuracy=0.9536372423171997>\n",
            "round 27, metrics=<num_examples=4745.0,loss=0.2139977514743805,accuracy=0.9386723041534424>\n",
            "round 28, metrics=<num_examples=4975.0,loss=0.14037224650382996,accuracy=0.9654271602630615>\n",
            "round 29, metrics=<num_examples=5410.0,loss=0.15738730132579803,accuracy=0.9597042798995972>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn70ix21PgDn",
        "colab_type": "code",
        "outputId": "117f75c1-645d-482c-c8f0-e7c1188cf29b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "source": [
        "'''\n",
        "#这个tensorBoard像有猫病一样，昨天还可以今天来突然就不行了\n",
        "#@test {\"skip\":true}\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /tmp/logs/scalars/ --port=0\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ERROR: Failed to launch TensorBoard (exited with 1).\n",
              "Contents of stderr:\n",
              "2020-04-06 08:24:12.865933: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
              "2020-04-06 08:24:12.866088: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
              "2020-04-06 08:24:12.866105: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
              "Traceback (most recent call last):\n",
              "  File \"/usr/local/bin/tensorboard\", line 8, in <module>\n",
              "    sys.exit(run_main())\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/main.py\", line 66, in run_main\n",
              "    app.run(tensorboard.main, flags_parser=tensorboard.configure)\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\n",
              "    _run_main(main, args)\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\n",
              "    sys.exit(main(argv))\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/program.py\", line 268, in main\n",
              "    return runner(self.flags) or 0\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/program.py\", line 282, in _run_serve_subcommand\n",
              "    server = self._make_server()\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/program.py\", line 361, in _make_server\n",
              "    self.assets_zip_provider)\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/backend/application.py\", line 165, in standard_tensorboard_wsgi\n",
              "    flags, plugin_loaders, data_provider, assets_zip_provider, multiplexer)\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/backend/application.py\", line 234, in TensorBoardWSGIApp\n",
              "    return TensorBoardWSGI(tbplugins, flags.path_prefix)\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/backend/application.py\", line 286, in __init__\n",
              "    raise ValueError('Duplicate plugins for name %s' % plugin.plugin_name)\n",
              "ValueError: Duplicate plugins for name whatif"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBGzrtDgP8hn",
        "colab_type": "code",
        "outputId": "0134db25-dc30-424a-fb8d-d97177b94010",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "#在测试集上进行测试\n",
        "#求值传递MnistModel就足够了，不执行梯度下降，也不需要构造优化器，MnistTModel就是前面定义的那个class\n",
        "evaluation = tff.learning.build_federated_evaluation(MnistModel)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pool2 Tensor(\"MaxPool2d_1:0\", shape=(None, 7, 7, 64), dtype=float32)\n",
            "flatten over\n",
            "dense1 over\n",
            "dense2 over\n",
            "y_pred Tensor(\"Softmax:0\", shape=(None, 10), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQjEUho65VGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#评估一下在训练中得到的最新状态，为了从服务器状态中提取最新的训练模型，只需访问.model成员\n",
        "#和上面说的形式一样，它接收model和联邦数据并返回训练的metrics\n",
        "#这里是在评估模型的最新状态，为了从服务器状态中提取最新的训练模型，只需访问.model成员\n",
        "train_metrics = evaluation(state.model, federated_train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M5UwerU5V6R",
        "colab_type": "code",
        "outputId": "54048b81-f29b-4d6e-dc91-ab5513ee629d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#编译联邦数据的测试样本，并对测试数据重新运行求值。同样也是随机选择NUM_CLIENTS个用户。\n",
        "# sample_clients = random.sample(emnist_train.client_ids, NUM_CLIENTS)\n",
        "# federated_test_data = make_federated_data(emnist_test, sample_clients)\n",
        "\n",
        "#BAL1测试精度\n",
        "sample_clients = random.sample(BAL1_test.client_ids, 5)\n",
        "federated_test_data = make_federated_data(BAL1_test, sample_clients)\n",
        "len(federated_test_data), federated_test_data[0]"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5,\n",
              " <PrefetchDataset shapes: OrderedDict([(x, (None, 28, 28, 1)), (y, (None, 1))]), types: OrderedDict([(x, tf.float32), (y, tf.int32)])>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a_F78e05cKM",
        "colab_type": "code",
        "outputId": "0af10f83-63b6-49e1-c9d2-ec69650c04c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#使用EMNIST，客户端随机选择10个，初始精度在0.8898\n",
        "#使用BAL1，客户端随机选了5个作为测试集测试，精度在0.9616666436195374\n",
        "test_metrics = evaluation(state.model, federated_test_data)\n",
        "\n",
        "str(test_metrics)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<num_examples=3000.0,loss=0.14176583290100098,accuracy=0.9616666436195374>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    }
  ]
}