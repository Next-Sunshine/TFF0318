{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestForTFFCopy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOhHrgAjbf2ARvCPJ/ZaT5Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Next-Sunshine/TFF0318/blob/master/TestForTFFCopy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9pTTi0lPh1w",
        "colab_type": "code",
        "outputId": "0a421791-0836-4eb0-be1c-7cba219cb403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "#环境测试\n",
        "#@test {\"skip\":true}\n",
        "!pip install --quiet --upgrade tensorflow_federated\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 430kB 1.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 44.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 53.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 20.0MB 215kB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8MB 44.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 10.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 421.8MB 37kB/s \n",
            "\u001b[K     |████████████████████████████████| 450kB 45.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.9MB 50.7MB/s \n",
            "\u001b[?25h  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITkxSUMiRa01",
        "colab_type": "code",
        "outputId": "2730105a-9056-4d97-8fa3-c0397ba5a529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "tf.compat.v1.enable_v2_behavior()\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "tff.federated_computation(lambda: 'Hello, world!')()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'Hello, world!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRJgOTiAR3T4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#装载数据集，实验使用EMNIST数据集\n",
        "emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHXBNT_UxIWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#此处得到客户端的总数K，因为每一轮随机选择C×K的向上取整\n",
        "#根据FedAvg做的研究C取0.2可获得较好的性能，为了简化模型K这里取固定的数\n",
        "K = len(emnist_train.client_ids)\n",
        "C = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc578aa2UuZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NUM_CLIENTS在实验的时候应该是K×C\n",
        "NUM_CLIENTS = 10\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 20\n",
        "SHUFFLE_BUFFER = 100\n",
        "PREFETCH_BUFFER = 10\n",
        "\n",
        "#预处理，将图片中的‘pixels'、‘label'分别表示成x和y\n",
        "#将28×28的图像展平成784个元素，打乱顺序\n",
        "def preprocess(dataset):\n",
        "  #内部函数将像素和标签转换成x和y，并将像素展平\n",
        "  def batch_format_fn(element):\n",
        "    return collections.OrderedDict(\n",
        "        #因为是灰度图所以通道数为1，如果是rgb图片则为3\n",
        "        #因为CNN卷积层接收一个4d向量\n",
        "        x = tf.reshape(element['pixels'], [-1,28,28,1]),\n",
        "        y = tf.reshape(element['label'], [-1,1])\n",
        "    )\n",
        "  \n",
        "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRqmbbzamP1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#为指定用户创建联邦数据,接收训练集和用户id\n",
        "def make_federated_data(client_data, client_ids):\n",
        "  return [\n",
        "      preprocess(client_data.create_tf_dataset_for_client(x))\n",
        "      for x in client_ids\n",
        "    ]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDHJSC8OYaZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#这里改成随机获得NUM_CLIENTS个用户，模拟联邦学习每轮的随机选择用户\n",
        " sample_clients = random.sample(emnist_train.client_ids, NUM_CLIENTS)\n",
        "# sample_clients #说明随机用户的id是取出来了的\n",
        "# type(emnist_train.client_ids)  #client_ids是一个列表\n",
        "\n",
        "federated_train_data = make_federated_data(emnist_train, sample_clients)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZEcia5FltoP",
        "colab_type": "text"
      },
      "source": [
        "间隔界"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4MRt7FEh2gq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "04bbd6ee-6d9a-487e-96db-9c7fb6dd1114"
      },
      "source": [
        "'''\n",
        "#！自己添加代码中，正在想办法把训练模型改成CNN\n",
        "class CNN(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(\n",
        "            filters=32,             # 卷积层神经元（卷积核）数目\n",
        "            kernel_size=[5, 5],     # 感受野大小\n",
        "            padding='same',         # padding策略（vaild 或 same）\n",
        "            activation=tf.nn.relu   # 激活函数\n",
        "        )\n",
        "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
        "        self.conv2 = tf.keras.layers.Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=[5, 5],\n",
        "            padding='same',\n",
        "            activation=tf.nn.relu\n",
        "        )\n",
        "        #print(\"kernel:\",kernel_size)\n",
        "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
        "        self.flatten = tf.keras.layers.Reshape(target_shape=(7 * 7 * 64,))\n",
        "        self.dense1 = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)\n",
        "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
        " \n",
        "    def call(self, inputs):\n",
        "        x = self.conv1(inputs)                  # [batch_size, 28, 28, 32]\n",
        "        x = self.pool1(x)                       # [batch_size, 14, 14, 32]\n",
        "        x = self.conv2(x)                       # [batch_size, 14, 14, 64]\n",
        "        x = self.pool2(x)                       # [batch_size, 7, 7, 64]\n",
        "        x = self.flatten(x)                     # [batch_size, 7 * 7 * 64]\n",
        "        x = self.dense1(x)                      # [batch_size, 1024]\n",
        "        x = self.dense2(x)\n",
        "        #tf.nn.softmax()将原始输出归一化，且能凸显原始向量中最大的值                      # [batch_size, 10]\n",
        "        output = tf.nn.softmax(x)\n",
        "        return output\n",
        "        '''\n"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#！自己添加代码中，正在想办法把训练模型改成CNN\\nclass CNN(tf.keras.Model):\\n    def __init__(self):\\n        super().__init__()\\n        self.conv1 = tf.keras.layers.Conv2D(\\n            filters=32,             # 卷积层神经元（卷积核）数目\\n            kernel_size=[5, 5],     # 感受野大小\\n            padding=\\'same\\',         # padding策略（vaild 或 same）\\n            activation=tf.nn.relu   # 激活函数\\n        )\\n        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\\n        self.conv2 = tf.keras.layers.Conv2D(\\n            filters=64,\\n            kernel_size=[5, 5],\\n            padding=\\'same\\',\\n            activation=tf.nn.relu\\n        )\\n        #print(\"kernel:\",kernel_size)\\n        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\\n        self.flatten = tf.keras.layers.Reshape(target_shape=(7 * 7 * 64,))\\n        self.dense1 = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)\\n        self.dense2 = tf.keras.layers.Dense(units=10)\\n \\n    def call(self, inputs):\\n        x = self.conv1(inputs)                  # [batch_size, 28, 28, 32]\\n        x = self.pool1(x)                       # [batch_size, 14, 14, 32]\\n        x = self.conv2(x)                       # [batch_size, 14, 14, 64]\\n        x = self.pool2(x)                       # [batch_size, 7, 7, 64]\\n        x = self.flatten(x)                     # [batch_size, 7 * 7 * 64]\\n        x = self.dense1(x)                      # [batch_size, 1024]\\n        x = self.dense2(x)\\n        #tf.nn.softmax()将原始输出归一化，且能凸显原始向量中最大的值                      # [batch_size, 10]\\n        output = tf.nn.softmax(x)\\n        return output\\n        '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw419GpLlmz9",
        "colab_type": "text"
      },
      "source": [
        "间隔界\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNo6O_5qSNSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#创建一个变量集合来表示所有变量,包括model(weights+bias)以及metrics(num_examples,loss_sum,accuracy_sum)\n",
        "# MnistVariables = collections.namedtuple(\n",
        "#     'MnistVariables','weights bias num_examples loss_sum accuracy_sum'\n",
        "# )\n",
        "MnistVariables = collections.namedtuple(\n",
        "    'MnistVariables','cnn_conv2d_kernel cnn_conv2d_bias cnn_conv2d_1_kernel cnn_conv2d_1_bias cnn_dense_kernel cnn_dense_bias cnn_dense_1_kernel cnn_dense_1_bias num_examples loss_sum accuracy_sum'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSEDLYxJTZ8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#创建变量并初始化\n",
        "def create_mnist_variables():\n",
        "  #核初始化函数，使用正态分布，先使用tf.XXX(初始参数)生成一个模型关键字C，再使用C(arg1)传参数进去\n",
        "  kernel_initalizer=tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05)\n",
        "  return MnistVariables(\n",
        "      #第一层卷积在5×5的patch中算出32个特征，权重张量形状为[5,5,1,32]\n",
        "      #前两个维度是patch的大小，接着是输入的通道数，最后是输出的通道数目\n",
        "      #h_conv1.shape=[-1,28,28,32]，第一个pooling层将其变成[-1,14,14,32]     \n",
        "      cnn_conv2d_kernel=tf.Variable(\n",
        "          #权重初始化至关重要，不能全部初始化为0！！！\n",
        "          initial_value=kernel_initalizer(shape=(5,5,1,32), dtype=tf.float32),\n",
        "          name='cnn_conv2d_kernel',\n",
        "          trainable=True),\n",
        "      #32个输出通道，每个输出通道都有一个对应的偏置\n",
        "      cnn_conv2d_bias=tf.Variable(\n",
        "          lambda: tf.zeros(dtype=tf.float32, shape=(32,)),\n",
        "          name='cnn_conv2d_bias',\n",
        "          trainable=True),\n",
        "      #第二层卷积,64个过滤器，共享权重矩阵为32*5*5,h_conv2.shape=[-1,14,14,64]\n",
        "      #第二个pooling层将其变成[-1,7,7,64]\n",
        "      cnn_conv2d_1_kernel=tf.Variable(\n",
        "          initial_value=kernel_initalizer(shape=(5,5,32,64), dtype=tf.float32),\n",
        "          name='cnn_conv2d_1_kernel',\n",
        "          trainable=True),\n",
        "      #64个输出通道对应64个偏置\n",
        "      cnn_conv2d_1_bias=tf.Variable(\n",
        "          lambda: tf.zeros(dtype=tf.float32, shape=(64,)),\n",
        "          name='cnn_conv2d_1_bias',\n",
        "          trainable=True),\n",
        "      #全连接层，现在图片尺寸减小到7×7，加入一个有1024个神经元的全连接层用于处理整个图片\n",
        "      #将池化层输出的张量reshape成一些7*7*64的向量，乘上权重加上偏置然后使用ReLU\n",
        "      cnn_dense_kernel=tf.Variable(\n",
        "          initial_value=kernel_initalizer(shape=(7*7*64,1024), dtype=tf.float32),\n",
        "          name='cnn_dense_kernel',\n",
        "          trainable=True),\n",
        "      cnn_dense_bias=tf.Variable(\n",
        "          lambda: tf.zeros(dtype=tf.float32, shape=(1024,)),\n",
        "          name='cnn_dense_bias',\n",
        "          trainable=True),\n",
        "      #这里以后可以加一个Dropout\n",
        "      cnn_dense_1_kernel=tf.Variable(\n",
        "          initial_value=kernel_initalizer(shape=(1024,10), dtype=tf.float32),\n",
        "          name='cnn_dense_1_kernel',\n",
        "          trainable=True),\n",
        "      cnn_dense_1_bias=tf.Variable(\n",
        "          lambda: tf.zeros(dtype=tf.float32, shape=(10,)),\n",
        "          name='cnn_dense_1_bias',\n",
        "          trainable=True),\n",
        "      num_examples=tf.Variable(0.0, name='num_examples', trainable=False),\n",
        "      loss_sum=tf.Variable(0.0, name='loss_sum', trainable=False),\n",
        "      accuracy_sum=tf.Variable(0.0, name='accuracy_sum', trainable=False)\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib6kQLwtUh5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#自定义前向传播函数\n",
        "def mnist_forward_pass(variables, batch):\n",
        "  _input_ = batch['x']  #取训练图片\n",
        "\n",
        "  #神经网络\n",
        "  #第一层卷积\n",
        "  conv = tf.nn.conv2d(_input_, variables.cnn_conv2d_kernel, strides=[1,1,1,1], padding='SAME')                  # [batch_size, 28, 28, 32]\n",
        "  pre_activation = tf.nn.bias_add(conv, variables.cnn_conv2d_bias)\n",
        "  conv1 = tf.nn.relu(pre_activation, name='conv1')\n",
        "\n",
        "  #第一层卷积层的池化\n",
        "  pool1 = tf.nn.max_pool2d(conv1, ksize=[2, 2], strides=2, padding='SAME')\n",
        "\n",
        "  #第二层卷积\n",
        "  conv = tf.nn.conv2d(pool1, variables.cnn_conv2d_1_kernel, strides=[1,1,1,1], padding='SAME')\n",
        "  pre_activation = tf.nn.bias_add(conv, variables.cnn_conv2d_1_bias)\n",
        "  conv2 = tf.nn.relu(pre_activation, name='conv2')\n",
        "\n",
        "  #第二层卷积的池化\n",
        "  pool2 = tf.nn.max_pool2d(conv2, ksize=[2,2], strides=2, padding='SAME')\n",
        "  print(\"pool2\",pool2)\n",
        "  \n",
        "  #flatten\n",
        "  flatten = tf.reshape(pool2, shape=(-1,7*7*64))\n",
        "  print(\"flatten over\")\n",
        "\n",
        "  #全连接层1\n",
        "  dense1 = tf.nn.relu(tf.matmul(flatten, variables.cnn_dense_kernel) + variables.cnn_dense_bias)\n",
        "  print(\"dense1 over\")\n",
        "\n",
        "  #全连接层2\n",
        "  dense2 = tf.matmul(dense1, variables.cnn_dense_1_kernel) + variables.cnn_dense_1_bias\n",
        "  print(\"dense2 over\")\n",
        "\n",
        "  y_pred = tf.nn.softmax(dense2)\n",
        "  print(\"y_pred\",y_pred)\n",
        "\n",
        "  #计算损失\n",
        "  flat_labels = tf.reshape(batch['y'], [-1])\n",
        "  loss = -tf.reduce_mean(\n",
        "      tf.reduce_sum(tf.one_hot(flat_labels, 10) * tf.math.log(y_pred), axis=[1]))\n",
        "  \n",
        "  \n",
        "  #计算准确率\n",
        "  predictions = tf.cast(tf.argmax(y_pred,1), tf.int32)\n",
        "  accuracy = tf.reduce_mean(\n",
        "      tf.cast(tf.equal(predictions, flat_labels), tf.float32))\n",
        "  \n",
        "  #样本数\n",
        "  num_examples = tf.cast(tf.size(batch['y']), tf.float32)\n",
        "\n",
        "  #更新样本数、损失和、精度和,每一批都考虑了自己的权重\n",
        "  variables.num_examples.assign_add(num_examples)\n",
        "  variables.loss_sum.assign_add(loss * num_examples)\n",
        "  variables.accuracy_sum.assign_add(accuracy * num_examples)\n",
        "\n",
        "  \n",
        "  return loss, predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mcpE0twbxz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#计算本地用户的metrics度量\n",
        "def get_local_mnist_metrics(variables):\n",
        "  return collections.OrderedDict(\n",
        "      num_examples=variables.num_examples,\n",
        "      loss=variables.loss_sum / variables.num_examples,\n",
        "      accuracy=variables.accuracy_sum / variables.num_examples\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv_a5zrVch3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#集合每个设备发出的本地度量\n",
        "@tff.federated_computation\n",
        "def aggregate_mnist_metrics_across_clients(metrics):\n",
        "  return collections.OrderedDict(\n",
        "      num_examples=tff.federated_sum(metrics.num_examples),\n",
        "      loss=tff.federated_mean(metrics.loss, metrics.num_examples),\n",
        "      accuracy=tff.federated_mean(metrics.accuracy, metrics.num_examples)\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE3FpcO5dNr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#自定义模型，创建tff.learning.model实例\n",
        "class MnistModel(tff.learning.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._variables = create_mnist_variables()\n",
        "\n",
        "  #所有的“tf.Variables”都应该在“__init__”中引入\n",
        "  @property\n",
        "  def trainable_variables(self):\n",
        "    #return [self._variables.weights, self._variables.bias]\n",
        "    return [self._variables.cnn_conv2d_kernel,\n",
        "        self._variables.cnn_conv2d_bias,\n",
        "        self._variables.cnn_conv2d_1_kernel,\n",
        "        self._variables.cnn_conv2d_1_bias,\n",
        "        self._variables.cnn_dense_kernel,\n",
        "        self._variables.cnn_dense_bias,\n",
        "        self._variables.cnn_dense_1_kernel,\n",
        "        self._variables.cnn_dense_1_bias\n",
        "        ]\n",
        "  \n",
        "  @property\n",
        "  def non_trainable_variables(self):\n",
        "    return []\n",
        "  \n",
        "  @property\n",
        "  def local_variables(self):\n",
        "    return [self._variables.num_examples, self._variables.loss_sum,\n",
        "         self._variables.accuracy_sum]\n",
        "  \n",
        "  @property\n",
        "  def input_spec(self):\n",
        "    return collections.OrderedDict(\n",
        "        x=tf.TensorSpec([None, 28,28,1], tf.float32),\n",
        "        y=tf.TensorSpec([None, 1], tf.int32)\n",
        "    )\n",
        "\n",
        "  @tf.function\n",
        "  def forward_pass(self, batch, training=True):\n",
        "    del training\n",
        "    loss, predictions = mnist_forward_pass(self._variables, batch)\n",
        "    #取出样本数\n",
        "    num_examples = tf.shape(batch['x'])[0]\n",
        "    return tff.learning.BatchOutput(\n",
        "        loss=loss, predictions=predictions, num_examples=num_examples)\n",
        "    \n",
        "  @tf.function\n",
        "  def report_local_outputs(self):\n",
        "    return get_local_mnist_metrics(self._variables)\n",
        "  \n",
        "  @property\n",
        "  def federated_output_computation(self):\n",
        "    return aggregate_mnist_metrics_across_clients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfqC79A4hVfw",
        "colab_type": "code",
        "outputId": "a31a8bbd-1a70-4f83-ed2a-0ba4cf03788b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "#创建迭代器执行联合平均的迭代过程\n",
        "iterative_process = tff.learning.build_federated_averaging_process(\n",
        "    MnistModel,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02)\n",
        ")"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pool2 Tensor(\"MaxPool2d_1:0\", shape=(None, 7, 7, 64), dtype=float32)\n",
            "flatten over\n",
            "dense1 over\n",
            "dense2 over\n",
            "y_pred Tensor(\"Softmax:0\", shape=(None, 10), dtype=float32)\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function zero_all_if_any_non_finite at 0x7f364253b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function zero_all_if_any_non_finite at 0x7f364253b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function zero_all_if_any_non_finite at 0x7f364253b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function zero_all_if_any_non_finite at 0x7f364253b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzJXeJNSlGpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#获得初始状态\n",
        "state = iterative_process.initialize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpUydYwQnfHd",
        "colab_type": "code",
        "outputId": "d4c35a0a-aaa8-48cc-fce6-ef6b5e30cfce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#查看第一轮训练\n",
        "sample_clients = random.sample(emnist_train.client_ids, NUM_CLIENTS)\n",
        "federated_train_data = make_federated_data(emnist_train, sample_clients)\n",
        "state, metrics = iterative_process.next(state, federated_train_data)\n",
        "print('round 1, metrics={}'.format(metrics))"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "round 1, metrics=<num_examples=4485.0,loss=2.3019957542419434,accuracy=0.11571906507015228>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkoGLYGLyYkC",
        "colab_type": "code",
        "outputId": "c1ed6a0e-dcd5-4de5-9cc1-1a1258c8bbd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "#计算更多轮\n",
        "NUM_ROUNDS = 20\n",
        "for round_num in range(2,NUM_ROUNDS):\n",
        "  #随机选择NUM_CLIENTS个用户\n",
        "  sample_clients = random.sample(emnist_train.client_ids, NUM_CLIENTS)\n",
        "  federated_train_data = make_federated_data(emnist_train, sample_clients)\n",
        "\n",
        "  state, metrics = iterative_process.next(state, federated_train_data)\n",
        "  print('round {:2d}, metrics={}'.format(round_num, metrics))"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "round  2, metrics=<num_examples=4860.0,loss=1.665802001953125,accuracy=0.565432071685791>\n",
            "round  3, metrics=<num_examples=5255.0,loss=1.3768004179000854,accuracy=0.6319695711135864>\n",
            "round  4, metrics=<num_examples=4800.0,loss=1.19942045211792,accuracy=0.6731250286102295>\n",
            "round  5, metrics=<num_examples=5180.0,loss=0.822767972946167,accuracy=0.7830115556716919>\n",
            "round  6, metrics=<num_examples=5240.0,loss=0.6395965814590454,accuracy=0.8198473453521729>\n",
            "round  7, metrics=<num_examples=5140.0,loss=0.6477411985397339,accuracy=0.8202334642410278>\n",
            "round  8, metrics=<num_examples=5165.0,loss=0.4296082854270935,accuracy=0.8913843035697937>\n",
            "round  9, metrics=<num_examples=4980.0,loss=0.4372885525226593,accuracy=0.8734939694404602>\n",
            "round 10, metrics=<num_examples=5430.0,loss=0.3003905415534973,accuracy=0.9250460267066956>\n",
            "round 11, metrics=<num_examples=4920.0,loss=0.3301681578159332,accuracy=0.9132114052772522>\n",
            "round 12, metrics=<num_examples=5345.0,loss=0.36047056317329407,accuracy=0.8963517546653748>\n",
            "round 13, metrics=<num_examples=5230.0,loss=0.30075159668922424,accuracy=0.9196940660476685>\n",
            "round 14, metrics=<num_examples=5200.0,loss=0.29581770300865173,accuracy=0.9171153903007507>\n",
            "round 15, metrics=<num_examples=5195.0,loss=0.24698896706104279,accuracy=0.9335899949073792>\n",
            "round 16, metrics=<num_examples=5165.0,loss=0.23276780545711517,accuracy=0.9442400932312012>\n",
            "round 17, metrics=<num_examples=5145.0,loss=0.26949208974838257,accuracy=0.927502453327179>\n",
            "round 18, metrics=<num_examples=5255.0,loss=0.19860416650772095,accuracy=0.9448144435882568>\n",
            "round 19, metrics=<num_examples=5280.0,loss=0.1471509486436844,accuracy=0.9628787636756897>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKKb6c6h0AMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#使用tensorboard可视化\n",
        "#使用Tensorboard可视化这些联邦计算的度量\n",
        "#创建目录和相应的摘要编写器\n",
        "#@test {\"skip\": true}\n",
        "logdir = \"/tmp/logs/scalars/training/\"\n",
        "summary_writer = tf.summary.create_file_writer(logdir)\n",
        "state = iterative_process.initialize()\n",
        "\n",
        "#!!!太坑了，我以为@test是无关紧要的东西，emmm，就省略了，没想到就凉凉\n",
        "#@test {\"skip\": true}\n",
        "with summary_writer.as_default():\n",
        "  for round_num in range(1, NUM_ROUNDS):\n",
        "    #随机选择用户\n",
        "    sample_clients = random.sample(emnist_train.client_ids, NUM_CLIENTS)\n",
        "    federated_train_data = make_federated_data(emnist_train, sample_clients)\n",
        "\n",
        "    state, metrics = iterative_process.next(state, federated_train_data)\n",
        "\n",
        "    print('round {:2d}, metrics={}'.format(round_num, metrics))\n",
        "\n",
        "    for name, value in metrics._asdict().items():\n",
        "      tf.summary.scalar(name, value, step=round_num)\n",
        "\n",
        "#这个tensorBoard像有猫病一样，昨天还可以今天来突然就不行了\n",
        "#@test {\"skip\":true}\n",
        "#%load_ext tensorboard\n",
        "%tensorboard --logdir /tmp/logs/scalars/ --port=0\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBGzrtDgP8hn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}