{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestForTFFCopy.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ip--KI1UhiSU",
        "eZEcia5FltoP",
        "pw419GpLlmz9",
        "i_EV-vx8b93f"
      ],
      "authorship_tag": "ABX9TyNT2Sli8YRNAevEzcX6u4my",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Next-Sunshine/TFF0318/blob/master/TestForTFFCopy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJVghqEic-bV",
        "colab_type": "text"
      },
      "source": [
        "### 导包"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9pTTi0lPh1w",
        "colab_type": "code",
        "outputId": "cd40f7bb-85bb-4f34-b5e6-c771be4357c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "#环境测试\n",
        "#@test {\"skip\":true}\n",
        "!pip install --quiet --upgrade tensorflow_federated\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 430kB 8.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 48.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 14.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 421.8MB 39kB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 42.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 20.0MB 160kB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8MB 47.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 450kB 53.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.9MB 50.1MB/s \n",
            "\u001b[?25h  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITkxSUMiRa01",
        "colab_type": "code",
        "outputId": "fe965323-2cfe-44f3-b902-4d868be86fc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import h5py\n",
        "import random\n",
        "import math\n",
        "import gc\n",
        "\n",
        "tf.compat.v1.enable_v2_behavior()\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "tff.federated_computation(lambda: 'Hello, world!')()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'Hello, world!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRJgOTiAR3T4",
        "colab_type": "code",
        "outputId": "9b08f773-6b76-409c-f9dc-60ee1dee227e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "#装载数据集，实验使用EMNIST数据集\n",
        "emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()\n",
        "#装载mnist数据集\n",
        "mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tff-datasets-public/fed_emnist_digitsonly.tar.bz2\n",
            "97402880/97398400 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p1XDcJPi_M-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#从mnist_train中读60000张图片放入一个数组中以备用\n",
        "class_ = [[]for i in range(10)]\n",
        "for i in range(len(mnist_train[1])):\n",
        "  label = mnist_train[1][i]\n",
        "  class_[label].append(mnist_train[0][i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy0gLL26EOv7",
        "colab_type": "text"
      },
      "source": [
        "### 构造BAL1数据集，标量平衡，全局平衡，局部IID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a39F3CRqX9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#当创建数据集需要修改的时候就要用这里，先清空再重新创建\n",
        "f1.flush()\n",
        "f1.clear()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz2h0TU23dsp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#设置各个不平衡的数据集，首先弄一个BAL1:10个类别，每个类别数目相等，每个用户都含有10个类别且每类数目相等\n",
        "#我调查的结果是每个label数目在5000-6000张之间，于是分给500个客户端差不多每个客户端均10个\n",
        "#创建自己的数据集首先是BAL1：全平衡，使用了mnist数据集来分配，60000张图片平均分给500个客户端，每个客户端分得10类，每类10张，供100张图片\n",
        "f1 = h5py.File(\"BAL1.hdf5\",\"a\")\n",
        "top_group = f1.create_group(\"examples\")   #创建顶层group名为examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkNHRXhikc8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MY_NUM_CLIENTS = 500  #写的时候最初用50，用500时间太长，试过500，磁盘会从30G飙升到60G且还运行不完，不知道后面500个客户端的时候怎么办\n",
        "NUM_PER_CLASS = 10  #每个客户端每类的图片数量\n",
        "#Collections.Orderdict->TensorSliceDataset+client_id->hdf5->HDF5ClientData\n",
        "#<TensorSliceDataset shapes: OrderedDict([(label, ()), (pixels, (28, 28))]), types: OrderedDict([(label, tf.int32), (pixels, tf.float32)])>\n",
        "pixels = []\n",
        "label=[]\n",
        "client_ids = []\n",
        "for i in range(MY_NUM_CLIENTS):  #i客户端数量\n",
        "  for j in range(10):  #j是类别数\n",
        "    for k in range(NUM_PER_CLASS): #k控制每一类图片放多少张在客户端\n",
        "      pixels.append(class_[j][i*NUM_PER_CLASS+k]/255.0) #转成0-1之内的数字再加入pixels，class_[j]存放的是第j类数\n",
        "      label.append(j)  #加入label\n",
        "  \n",
        "  client_ids.append(\"f00_\" + str(i))\n",
        "  temp_group = top_group.create_group(name=\"f00_\" + str(i))  #创建以client_id为名字的group\n",
        "  label_ds = temp_group.create_dataset(name=\"label\", data=np.array(label, dtype='int32'))  #向group中写入label\n",
        "  pixels_ds = temp_group.create_dataset(name=\"pixels\", data=np.array(pixels, dtype='float32'))  #向group中写入pixels\n",
        "\n",
        "  #清空上一个pixels数组和label数组中存放的上一个客户端的数据信息,否则会有灾难！！！\n",
        "  pixels.clear()\n",
        "  label.clear()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sTtobq61DNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#数据集构造完了以后，调用内存清理操作\n",
        "del pixels[:]\n",
        "del label[:]\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1rl-timDodS",
        "colab_type": "text"
      },
      "source": [
        "### 构造BAL2数据集：标量平衡且全局平衡，局部Non-IID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk85zh4PEBah",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3120c44e-2706-4591-b096-a03e6c2e58ec"
      },
      "source": [
        "#构造BAL2\n",
        "'''\n",
        "标量平衡:每个本地100张\n",
        "全局平衡：共10个类，每个类10张\n",
        "局部平衡：随机\n",
        "num_clients = 500\n",
        "'''"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n标量平衡:每个本地100张\\n全局平衡：共10个类，每个类10张\\n局部平衡：随机\\nnum_clients = 500\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4BSk6uLEqZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#构造绝对平衡图片集,10张*10类*500个客户端\n",
        "#因为后面构造数据集的时候对bal2_balance_class进行了删除操作，因此每次生成数据集都要重新生成bal2_balance_class\n",
        "bal2_balance_class = [[]for i in range(10)]  \n",
        "for i in range(0,10):\n",
        "  bal2_balance_class[i] = class_[i][0:5000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbEk8BCPEx4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#删除一个h5py文件\n",
        "f2.flush()\n",
        "f2.clear()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLhuN4Z8E1wD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#创建hdf5文件\n",
        "f2 = h5py.File(\"BAL2.hdf5\",\"a\")\n",
        "#创建顶层group\n",
        "top_group = f2.create_group(\"examples\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bODwz4MPE-gO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#创建BAL2:使用了洗牌算法\n",
        "#将10个类别平均放到500个客户端中，每个客户端每个类别放12个\n",
        "import random\n",
        "NUM_CLIENTS = 500  #写的时候最初用50，用500时间太长\n",
        "NUM_CLASS = 10\n",
        "NUM_PER_CLASS = 10\n",
        "\n",
        "#label里面有很多label，pixels里面有很多个pixels\n",
        "#浅拷贝temp_balance_class = balance_class修改temp的值原来的值也会变，不适合\n",
        "#使用深拷贝将balance_class里面的元素拷贝出来，\n",
        "#temp_balance_class = np.copy(balance_class)\n",
        "pixels = []\n",
        "label=[]\n",
        "client_ids = []\n",
        "for i in range(NUM_CLIENTS):  #i客户端数量\n",
        "  j = 0\n",
        "  #j是每个客户端控制取多少张图片\n",
        "  #使用while循环方便控制当选到label是i但是i已经被之前的客户端选择完了的情况，增设j控制\n",
        "  while(j<NUM_CLASS*NUM_PER_CLASS):\n",
        "    index_label = random.randint(0, NUM_CLASS-1) #随机产生一个label下标\n",
        "    num_remain_pixels = len(bal2_balance_class[index_label])\n",
        "    if(num_remain_pixels>0):\n",
        "      index_pixels = random.randint(0,num_remain_pixels-1)  #在label剩下的数中随机产生一个pixels下标\n",
        "      j = j+1\n",
        "    else:\n",
        "      continue\n",
        "    pixels.append(bal2_balance_class[index_label][index_pixels]/255.0) #转成0-1之内的数字再加入pixels\n",
        "    label.append(index_label)  #加入label\n",
        "    del bal2_balance_class[index_label][index_pixels] #使用下标删除元素,但是不能删除array的元素\n",
        "  \n",
        "  client_ids.append(\"f00_\" + str(i))\n",
        "  temp_group = top_group.create_group(name=\"f00_\" + str(i))\n",
        "  label_ds = temp_group.create_dataset(name=\"label\", data=np.array(label, dtype='int32'))\n",
        "  pixels_ds = temp_group.create_dataset(name=\"pixels\", data=np.array(pixels, dtype='float32'))\n",
        "  pixels.clear()\n",
        "  label.clear()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDhERs0e1a2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#数据集构造完成以后，调用内存清理操作\n",
        "del pixels[:]\n",
        "del label[:]\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0kylMRgyWG_",
        "colab_type": "text"
      },
      "source": [
        "### 构造BAL3数据集：全局平衡，标量不平衡，局部Non-IID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzfySSTPyovy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "构造BAL3数据集\n",
        "全局平衡：10个label，每个label5000张图片\n",
        "标量不平衡，局部Non-IID\n",
        "客户端：500，C=0.02\n",
        "50000张图片随机分给500个用户\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqUJ4M8uzr-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#获得全局绝对平衡数据集\n",
        "bal3_balance_class = [[]for i in range(10)]  \n",
        "for i in range(0,10):\n",
        "  bal3_balance_class[i] = class_[i][0:5000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUSHrgG9zvH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#删除一个h5py文件\n",
        "f3.flush()\n",
        "f3.clear()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPmY8-G3zxnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#创建hdf5文件\n",
        "f3 = h5py.File(\"BAL3.hdf5\",\"a\")\n",
        "#创建顶层group\n",
        "top_group = f3.create_group(\"examples\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt6u9CLTz3Wl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#创建BAL3\n",
        "#将50000张图片随机分给500个客户端\n",
        "\n",
        "NUM_CLIENTS = 500  #写的时候最初用50，用500时间太长\n",
        "NUM_CLASS = 10\n",
        "NUM_PER_CLASS = 10\n",
        "\n",
        "\n",
        "pixels = [[]for i in range(NUM_CLIENTS)]\n",
        "label = [[]for i in range(NUM_CLIENTS)]\n",
        "client_ids = []\n",
        "for i in range(NUM_CLASS):  #循环10*5000\n",
        "  for j in range(len(bal3_balance_class[i])):\n",
        "    #随机产生一个客户端的下标然后将该图片给放进去\n",
        "    random_client_id = random.randint(0, NUM_CLIENTS-1)\n",
        "    pixels[random_client_id].append(bal3_balance_class[i][j]/255.0)\n",
        "    label[random_client_id].append(i)\n",
        "    \n",
        "#将pixels和label加入对应client的group中\n",
        "for i in range(NUM_CLIENTS):\n",
        "  client_ids.append(\"f00_\" + str(i))\n",
        "  temp_group = top_group.create_group(name=\"f00_\" + str(i))\n",
        "  label_ds = temp_group.create_dataset(name=\"label\", data=np.array(label[i], dtype='int32'))\n",
        "  pixels_ds = temp_group.create_dataset(name=\"pixels\", data=np.array(pixels[i], dtype='float32'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFssf18G1qka",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "93aa5714-d58f-4b1c-8b16-54ec33c61686"
      },
      "source": [
        "#数据集构造完成以后，调用内存清理操作\n",
        "del pixels[:]\n",
        "del label[:]\n",
        "del bal3_balance_class[:]\n",
        "gc.collect()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEoCrfZQg6Qf",
        "colab_type": "text"
      },
      "source": [
        "### 构造BAL4数据集，全局不平衡，标量不平衡，局部Non-IID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L66Q84ZZhXYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "BAL4：标量不平衡，全局不平衡，局部不平衡\n",
        "使用原始的mnist\n",
        "60000张图片随机分给500个客户端，每个客户端数目不固定\n",
        "整体设置为病态不平衡，标量从40-220不等，局部Non-IID也非常病态\n",
        "由STEP控制不平衡程度\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjWO678VhbGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#获取总的数据集\n",
        "bal4_imbalance_class = class_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoCoTfYphdkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#删除一个h5py文件\n",
        "f4.flush()\n",
        "f4.clear()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLVp8IHMhgDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#创建hdf5文件\n",
        "f4 = h5py.File(\"BAL4.hdf5\",\"a\")\n",
        "#创建顶层group\n",
        "top_group = f4.create_group(\"examples\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd-XISWIhjB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#创建BAL4\n",
        "#将60000张图片随机分给500个客户端，为了达到病态的效果，以STEP张图片为一个碎片，将碎片随机分给500个客户端\n",
        "NUM_CLIENTS = 500  #写的时候最初用50，用500时间太长\n",
        "NUM_CLASS = 10\n",
        "NUM_PER_CLASS = 10\n",
        "STEP = 10 #如果以10张图片为一个碎片，病态不平衡的体现，STEP越大越不平衡\n",
        "\n",
        "pixels = [[]for i in range(NUM_CLIENTS)]\n",
        "label = [[]for i in range(NUM_CLIENTS)]\n",
        "client_ids = []\n",
        "for i in range(NUM_CLASS):  #循环10*5000\n",
        "  for j in range(0,len(bal4_imbalance_class[i]),STEP):\n",
        "    #随机产生一个客户端的下标然后将图片给放进去\n",
        "    random_client_id = random.randint(0, NUM_CLIENTS-1)\n",
        "    for k in range(min(STEP, len(bal4_imbalance_class[i])-j)):\n",
        "      pixels[random_client_id].append(bal4_imbalance_class[i][j+k]/255.0)\n",
        "      label[random_client_id].append(i)\n",
        "\n",
        "for i in range(NUM_CLIENTS):\n",
        "  client_ids.append(\"f00_\" + str(i))\n",
        "  temp_group = top_group.create_group(name=\"f00_\" + str(i))\n",
        "  label_ds = temp_group.create_dataset(name=\"label\", data=np.array(label[i], dtype='int32'))\n",
        "  pixels_ds = temp_group.create_dataset(name=\"pixels\", data=np.array(pixels[i], dtype='float32'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN78Iq1mhoWJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b5ffdcd9-9e82-4dca-cdd6-5b30d287845f"
      },
      "source": [
        "#数据集构造完成以后，调用内存清理操作\n",
        "del pixels[:]\n",
        "del label[:]\n",
        "del bal4_imbalance_class[:]\n",
        "gc.collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFa-6jkwEmYA",
        "colab_type": "text"
      },
      "source": [
        "### 各个数据集实例化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLcRrX6pk41S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#实例化构造的数据集：BAL1\n",
        "BAL1 = tff.simulation.hdf5_client_data.HDF5ClientData(\"BAL1.hdf5\")\n",
        "num_clients_BAL1 = len(BAL1.client_ids)\n",
        "#留出总客户端的4分之一做测试集\n",
        "BAL1_train, BAL1_test = BAL1.train_test_client_split(client_data=BAL1, num_test_clients= num_clients_BAL1//4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTobUHpCFcHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#实例化构造的数据集：BAL2\n",
        "BAL2 = tff.simulation.hdf5_client_data.HDF5ClientData(\"BAL2.hdf5\")\n",
        "num_clients_BAL2 = len(BAL2.client_ids)\n",
        "#留出总客户端的4分之一做测试集\n",
        "BAL2_train, BAL2_test = BAL2.train_test_client_split(client_data=BAL2, num_test_clients= num_clients_BAL2//4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeyvo_YQ2fBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#实例化构造的数据集：BAL3\n",
        "BAL3 = tff.simulation.hdf5_client_data.HDF5ClientData(\"BAL3.hdf5\")\n",
        "num_clients_BAL3 = len(BAL3.client_ids)\n",
        "#留出总客户端的4分之一做测试集\n",
        "BAL3_train, BAL3_test = BAL3.train_test_client_split(client_data=BAL3, num_test_clients= num_clients_BAL3//4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGl0f9fVwy4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#实例化构造的数据集：BAL4\n",
        "BAL4 = tff.simulation.hdf5_client_data.HDF5ClientData(\"BAL4.hdf5\")\n",
        "num_clients_BAL4 = len(BAL4.client_ids)\n",
        "#留出总客户端的4分之一做测试集\n",
        "BAL4_train, BAL4_test = BAL4.train_test_client_split(client_data=BAL4, num_test_clients= num_clients_BAL4//4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKth4b_uidw3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "df423ba2-0666-4c1b-8cba-b79562d1027a"
      },
      "source": [
        "print(len(BAL4_train.client_ids))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPD6BU2unLwo",
        "colab_type": "code",
        "outputId": "0bd01f79-6dc7-4dfb-d448-13ac4dcba431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#BAL1.client_ids\n",
        "#这里面的每一个打印出来都是和example_dataset类型一样的\n",
        "\n",
        "x = tf.data.Dataset.from_tensor_slices(\n",
        "        collections.OrderedDict((name, ds.value) for name, ds in sorted(\n",
        "            f1[\"examples\"][BAL1.client_ids[0]].items())))\n",
        "print(x)\n",
        "example_dataset = emnist_train.create_tf_dataset_for_client(\n",
        "    emnist_train.client_ids[0]\n",
        ")\n",
        "print(example_dataset)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<TensorSliceDataset shapes: OrderedDict([(label, ()), (pixels, (28, 28))]), types: OrderedDict([(label, tf.int32), (pixels, tf.float32)])>\n",
            "<TensorSliceDataset shapes: OrderedDict([(label, ()), (pixels, (28, 28))]), types: OrderedDict([(label, tf.int32), (pixels, tf.float32)])>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHXBNT_UxIWo",
        "colab_type": "code",
        "outputId": "a234c1c7-85da-4ce0-f7bf-4e8dab10d3af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#此处得到客户端的总数K，因为每一轮随机选择C×K的向上取整\n",
        "#根据FedAvg做的研究C取0.2可获得较好的性能，为了简化模型K这里取固定的数\n",
        "# K = len(emnist_train.client_ids)\n",
        "#BAL1数据集\n",
        "K = len(BAL4_train.client_ids)\n",
        "C = 0.02\n",
        "math.ceil(K*C)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaO3Jj0e8Pwb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "f9120ed8-ff25-4a7c-dbba-3cea2b1af56e"
      },
      "source": [
        "#这个单元格在看数据集里面的图片是否正常\n",
        "example_dataset = BAL4_train.create_tf_dataset_for_client(\n",
        "    BAL4_train.client_ids[9]\n",
        ")\n",
        "\n",
        "example_element = next(iter(example_dataset))\n",
        "print(example_element['label'].numpy())  #加了.numpy（）是在取numpy()属性的值，就是用.取属性值的意思\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.imshow(example_element['pixels'].numpy(), cmap='gray', aspect='equal')\n",
        "plt.grid(False)\n",
        "_=plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSRB2IIn/AA9eEl6Mb/7IAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip--KI1UhiSU",
        "colab_type": "text"
      },
      "source": [
        "### 图像预处理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc578aa2UuZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NUM_CLIENTS在实验的时候应该是K×C,375*0.2=75\n",
        "#NUM_CLIENTS = math.ceil(K*C)\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 20\n",
        "SHUFFLE_BUFFER = 100\n",
        "PREFETCH_BUFFER = 10\n",
        "\n",
        "#预处理，将图片中的‘pixels'、‘label'分别表示成x和y\n",
        "#将28×28的图像展平成784个元素，打乱顺序\n",
        "def preprocess(dataset):\n",
        "  #内部函数将像素和标签转换成x和y，并将像素展平\n",
        "  def batch_format_fn(element):\n",
        "    return collections.OrderedDict(\n",
        "        #因为是灰度图所以通道数为1，如果是rgb图片则为3\n",
        "        #因为CNN卷积层接收一个4d向量\n",
        "        x = tf.reshape(element['pixels'], [-1,28,28,1]),\n",
        "        y = tf.reshape(element['label'], [-1,1])\n",
        "    )\n",
        "  \n",
        "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRqmbbzamP1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#为指定用户创建联邦数据,接收训练集和用户id\n",
        "def make_federated_data(client_data, client_ids):\n",
        "  return [\n",
        "      preprocess(client_data.create_tf_dataset_for_client(x))\n",
        "      for x in client_ids\n",
        "    ]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDHJSC8OYaZj",
        "colab_type": "code",
        "outputId": "1abc32fc-b416-4a87-f2d0-3e5080d4ca02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "#这里改成随机获得NUM_CLIENTS个用户，模拟联邦学习每轮的随机选择用户\n",
        "# sample_clients = random.sample(emnist_train.client_ids, NUM_CLIENTS)\n",
        "# federated_train_data = make_federated_data(emnist_train, sample_clients)\n",
        "example_dataset = BAL1.create_tf_dataset_for_client(\n",
        "    BAL1.client_ids[0]\n",
        ")\n",
        "print(\"examle_dataset\",example_dataset)\n",
        "\n",
        "#BAL1数据集测试\n",
        "#sample_clients = random.sample(BAL1.client_ids, NUM_CLIENTS)\n",
        "sample_clients = BAL1_train.client_ids[0:NUM_CLIENTS]\n",
        "federated_train_data = make_federated_data(BAL1, sample_clients)\n",
        "\n",
        "#BAL2数据集测试\n",
        "sample_clients = BAL1_train.client_ids[0:NUM_CLIENTS]\n",
        "federated_train_data = make_federated_data(BAL1, sample_clients)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-6fef7b7e920e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m example_dataset = BAL1.create_tf_dataset_for_client(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mBAL1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"examle_dataset\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexample_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BAL1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZEcia5FltoP",
        "colab_type": "text"
      },
      "source": [
        "### 原使用class的模型部分"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4MRt7FEh2gq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#！自己添加代码中，正在想办法把训练模型改成CNN\n",
        "class CNN(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(\n",
        "            filters=32,             # 卷积层神经元（卷积核）数目\n",
        "            kernel_size=[5, 5],     # 感受野大小\n",
        "            padding='same',         # padding策略（vaild 或 same）\n",
        "            activation=tf.nn.relu   # 激活函数\n",
        "        )\n",
        "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
        "        self.conv2 = tf.keras.layers.Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=[5, 5],\n",
        "            padding='same',\n",
        "            activation=tf.nn.relu\n",
        "        )\n",
        "        #print(\"kernel:\",kernel_size)\n",
        "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
        "        self.flatten = tf.keras.layers.Reshape(target_shape=(7 * 7 * 64,))\n",
        "        self.dense1 = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)\n",
        "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
        " \n",
        "    def call(self, inputs):\n",
        "        x = self.conv1(inputs)                  # [batch_size, 28, 28, 32]\n",
        "        x = self.pool1(x)                       # [batch_size, 14, 14, 32]\n",
        "        x = self.conv2(x)                       # [batch_size, 14, 14, 64]\n",
        "        x = self.pool2(x)                       # [batch_size, 7, 7, 64]\n",
        "        x = self.flatten(x)                     # [batch_size, 7 * 7 * 64]\n",
        "        x = self.dense1(x)                      # [batch_size, 1024]\n",
        "        x = self.dense2(x)\n",
        "        #tf.nn.softmax()将原始输出归一化，且能凸显原始向量中最大的值                      # [batch_size, 10]\n",
        "        output = tf.nn.softmax(x)\n",
        "        return output\n",
        "        '''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw419GpLlmz9",
        "colab_type": "text"
      },
      "source": [
        "### 模型定义部分\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNo6O_5qSNSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#创建一个变量集合来表示所有变量,包括model(weights+bias)以及metrics(num_examples,loss_sum,accuracy_sum)\n",
        "MnistVariables = collections.namedtuple(\n",
        "    'MnistVariables','cnn_conv2d_kernel cnn_conv2d_bias cnn_conv2d_1_kernel cnn_conv2d_1_bias cnn_dense_kernel cnn_dense_bias cnn_dense_1_kernel cnn_dense_1_bias num_examples loss_sum accuracy_sum'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSEDLYxJTZ8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#创建变量并初始化\n",
        "def create_mnist_variables():\n",
        "  #核初始化函数，使用正态分布，先使用tf.XXX(初始参数)生成一个模型关键字C，再使用C(arg1)传参数进去\n",
        "  kernel_initalizer=tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05)\n",
        "  return MnistVariables(\n",
        "      #第一层卷积在5×5的patch中算出32个特征，权重张量形状为[5,5,1,32]\n",
        "      #前两个维度是patch的大小，接着是输入的通道数，最后是输出的通道数目\n",
        "      #h_conv1.shape=[-1,28,28,32]，第一个pooling层将其变成[-1,14,14,32]     \n",
        "      cnn_conv2d_kernel=tf.Variable(\n",
        "          #权重初始化至关重要，不能全部初始化为0！！！\n",
        "          initial_value=kernel_initalizer(shape=(5,5,1,32), dtype=tf.float32),\n",
        "          name='cnn_conv2d_kernel',\n",
        "          trainable=True),\n",
        "      #32个输出通道，每个输出通道都有一个对应的偏置\n",
        "      cnn_conv2d_bias=tf.Variable(\n",
        "          lambda: tf.zeros(dtype=tf.float32, shape=(32,)),\n",
        "          name='cnn_conv2d_bias',\n",
        "          trainable=True),\n",
        "      #第二层卷积,64个过滤器，共享权重矩阵为32*5*5,h_conv2.shape=[-1,14,14,64]\n",
        "      #第二个pooling层将其变成[-1,7,7,64]\n",
        "      cnn_conv2d_1_kernel=tf.Variable(\n",
        "          initial_value=kernel_initalizer(shape=(5,5,32,64), dtype=tf.float32),\n",
        "          name='cnn_conv2d_1_kernel',\n",
        "          trainable=True),\n",
        "      #64个输出通道对应64个偏置\n",
        "      cnn_conv2d_1_bias=tf.Variable(\n",
        "          lambda: tf.zeros(dtype=tf.float32, shape=(64,)),\n",
        "          name='cnn_conv2d_1_bias',\n",
        "          trainable=True),\n",
        "      #全连接层，现在图片尺寸减小到7×7，加入一个有1024个神经元的全连接层用于处理整个图片\n",
        "      #将池化层输出的张量reshape成一些7*7*64的向量，乘上权重加上偏置然后使用ReLU\n",
        "      cnn_dense_kernel=tf.Variable(\n",
        "          initial_value=kernel_initalizer(shape=(7*7*64,1024), dtype=tf.float32),\n",
        "          name='cnn_dense_kernel',\n",
        "          trainable=True),\n",
        "      cnn_dense_bias=tf.Variable(\n",
        "          lambda: tf.zeros(dtype=tf.float32, shape=(1024,)),\n",
        "          name='cnn_dense_bias',\n",
        "          trainable=True),\n",
        "      #这里以后可以加一个Dropout\n",
        "      cnn_dense_1_kernel=tf.Variable(\n",
        "          initial_value=kernel_initalizer(shape=(1024,10), dtype=tf.float32),\n",
        "          name='cnn_dense_1_kernel',\n",
        "          trainable=True),\n",
        "      cnn_dense_1_bias=tf.Variable(\n",
        "          lambda: tf.zeros(dtype=tf.float32, shape=(10,)),\n",
        "          name='cnn_dense_1_bias',\n",
        "          trainable=True),\n",
        "      num_examples=tf.Variable(0.0, name='num_examples', trainable=False),\n",
        "      loss_sum=tf.Variable(0.0, name='loss_sum', trainable=False),\n",
        "      accuracy_sum=tf.Variable(0.0, name='accuracy_sum', trainable=False)\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib6kQLwtUh5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#自定义前向传播函数\n",
        "def mnist_forward_pass(variables, batch):\n",
        "  _input_ = batch['x']  #取训练图片\n",
        "\n",
        "  #神经网络\n",
        "  #第一层卷积\n",
        "  conv = tf.nn.conv2d(_input_, variables.cnn_conv2d_kernel, strides=[1,1,1,1], padding='SAME')                  # [batch_size, 28, 28, 32]\n",
        "  pre_activation = tf.nn.bias_add(conv, variables.cnn_conv2d_bias)\n",
        "  conv1 = tf.nn.relu(pre_activation, name='conv1')\n",
        "\n",
        "  #第一层卷积层的池化\n",
        "  pool1 = tf.nn.max_pool2d(conv1, ksize=[2, 2], strides=2, padding='SAME')\n",
        "\n",
        "  #第二层卷积\n",
        "  conv = tf.nn.conv2d(pool1, variables.cnn_conv2d_1_kernel, strides=[1,1,1,1], padding='SAME')\n",
        "  pre_activation = tf.nn.bias_add(conv, variables.cnn_conv2d_1_bias)\n",
        "  conv2 = tf.nn.relu(pre_activation, name='conv2')\n",
        "\n",
        "  #第二层卷积的池化\n",
        "  pool2 = tf.nn.max_pool2d(conv2, ksize=[2,2], strides=2, padding='SAME')\n",
        "  print(\"pool2\",pool2)\n",
        "  \n",
        "  #flatten\n",
        "  flatten = tf.reshape(pool2, shape=(-1,7*7*64))\n",
        "  print(\"flatten over\")\n",
        "\n",
        "  #全连接层1\n",
        "  dense1 = tf.nn.relu(tf.matmul(flatten, variables.cnn_dense_kernel) + variables.cnn_dense_bias)\n",
        "  print(\"dense1 over\")\n",
        "\n",
        "  #添加一个Dropout防止过拟合\n",
        "  #keep_drop = tf.placeholder(tf.float32)\n",
        "  #drop_dense1 = tf.nn.tropout(dense1, variables.keep_drop)\n",
        "\n",
        "  #全连接层2\n",
        "  dense2 = tf.matmul(dense1, variables.cnn_dense_1_kernel) + variables.cnn_dense_1_bias\n",
        "  print(\"dense2 over\")\n",
        "\n",
        "  y_pred = tf.nn.softmax(dense2)\n",
        "  print(\"y_pred\",y_pred)\n",
        "\n",
        "  #计算损失\n",
        "  flat_labels = tf.reshape(batch['y'], [-1])\n",
        "  loss = -tf.reduce_mean(\n",
        "      tf.reduce_sum(tf.one_hot(flat_labels, 10) * tf.math.log(y_pred), axis=[1]))\n",
        "  \n",
        "  \n",
        "  #计算准确率\n",
        "  predictions = tf.cast(tf.argmax(y_pred,1), tf.int32)\n",
        "  accuracy = tf.reduce_mean(\n",
        "      tf.cast(tf.equal(predictions, flat_labels), tf.float32))\n",
        "  \n",
        "  #样本数\n",
        "  num_examples = tf.cast(tf.size(batch['y']), tf.float32)\n",
        "\n",
        "  #更新样本数、损失和、精度和,每一批都考虑了自己的权重\n",
        "  variables.num_examples.assign_add(num_examples)\n",
        "  variables.loss_sum.assign_add(loss * num_examples)\n",
        "  variables.accuracy_sum.assign_add(accuracy * num_examples)\n",
        "\n",
        "  \n",
        "  return loss, predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mcpE0twbxz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#计算本地用户的metrics度量\n",
        "def get_local_mnist_metrics(variables):\n",
        "  return collections.OrderedDict(\n",
        "      num_examples=variables.num_examples,\n",
        "      loss=variables.loss_sum / variables.num_examples,\n",
        "      accuracy=variables.accuracy_sum / variables.num_examples\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv_a5zrVch3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#集合每个设备发出的本地度量\n",
        "@tff.federated_computation\n",
        "def aggregate_mnist_metrics_across_clients(metrics):\n",
        "  return collections.OrderedDict(\n",
        "      num_examples=tff.federated_sum(metrics.num_examples),\n",
        "      loss=tff.federated_mean(metrics.loss, metrics.num_examples),\n",
        "      accuracy=tff.federated_mean(metrics.accuracy, metrics.num_examples)\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE3FpcO5dNr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#自定义模型，创建tff.learning.model实例\n",
        "class MnistModel(tff.learning.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._variables = create_mnist_variables()\n",
        "\n",
        "  #所有的“tf.Variables”都应该在“__init__”中引入\n",
        "  @property\n",
        "  def trainable_variables(self):\n",
        "    #return [self._variables.weights, self._variables.bias]\n",
        "    return [self._variables.cnn_conv2d_kernel,\n",
        "        self._variables.cnn_conv2d_bias,\n",
        "        self._variables.cnn_conv2d_1_kernel,\n",
        "        self._variables.cnn_conv2d_1_bias,\n",
        "        self._variables.cnn_dense_kernel,\n",
        "        self._variables.cnn_dense_bias,\n",
        "        self._variables.cnn_dense_1_kernel,\n",
        "        self._variables.cnn_dense_1_bias\n",
        "        ]\n",
        "  \n",
        "  @property\n",
        "  def non_trainable_variables(self):\n",
        "    return []\n",
        "  \n",
        "  @property\n",
        "  def local_variables(self):\n",
        "    return [self._variables.num_examples, self._variables.loss_sum,\n",
        "         self._variables.accuracy_sum]\n",
        "  \n",
        "  @property\n",
        "  def input_spec(self):\n",
        "    return collections.OrderedDict(\n",
        "        x=tf.TensorSpec([None, 28,28,1], tf.float32),\n",
        "        y=tf.TensorSpec([None, 1], tf.int32)\n",
        "    )\n",
        "\n",
        "  @tf.function\n",
        "  def forward_pass(self, batch, training=True):\n",
        "    del training\n",
        "    loss, predictions = mnist_forward_pass(self._variables, batch)\n",
        "    #取出样本数\n",
        "    num_examples = tf.shape(batch['x'])[0]\n",
        "    return tff.learning.BatchOutput(\n",
        "        loss=loss, predictions=predictions, num_examples=num_examples)\n",
        "    \n",
        "  @tf.function\n",
        "  def report_local_outputs(self):\n",
        "    return get_local_mnist_metrics(self._variables)\n",
        "  \n",
        "  @property\n",
        "  def federated_output_computation(self):\n",
        "    return aggregate_mnist_metrics_across_clients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfqC79A4hVfw",
        "colab_type": "code",
        "outputId": "e1848aa6-010d-4b59-ddff-c255351b5a9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "#创建迭代器执行联合平均的迭代过程\n",
        "iterative_process = tff.learning.build_federated_averaging_process(\n",
        "    MnistModel,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02)\n",
        ")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pool2 Tensor(\"MaxPool2d_1:0\", shape=(None, 7, 7, 64), dtype=float32)\n",
            "flatten over\n",
            "dense1 over\n",
            "dense2 over\n",
            "y_pred Tensor(\"Softmax:0\", shape=(None, 10), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77pwbgWvcIPH",
        "colab_type": "text"
      },
      "source": [
        "### 模型训练部分"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzJXeJNSlGpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#获得初始状态\n",
        "state = iterative_process.initialize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpUydYwQnfHd",
        "colab_type": "code",
        "outputId": "b218d94d-0f8f-499c-c2b0-9671ca53ccd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "NUM_CLIENTS_PER_ROUND = math.ceil(K * C)\n",
        "#查看第一轮训练\n",
        "# sample_clients = random.sample(emnist_train.client_ids, NUM_CLIENTS)\n",
        "# federated_train_data = make_federated_data(emnist_train, sample_clients)\n",
        "#BAL1数据集测试\n",
        "# sample_clients = random.sample(BAL1_train.client_ids, NUM_CLIENTS_PER_ROUND)\n",
        "# federated_train_data = make_federated_data(BAL1_train, sample_clients)\n",
        "#BAL2数据集测试\n",
        "# sample_clients = random.sample(BAL2_train.client_ids, NUM_CLIENTS_PER_ROUND)\n",
        "# federated_train_data = make_federated_data(BAL2_train, sample_clients)\n",
        "\n",
        "# #BAL3数据集测试\n",
        "# sample_clients = random.sample(BAL3_train.client_ids, NUM_CLIENTS_PER_ROUND)\n",
        "# federated_train_data = make_federated_data(BAL3_train, sample_clients)\n",
        "\n",
        "#BAL4数据集测试\n",
        "sample_clients = random.sample(BAL4_train.client_ids, NUM_CLIENTS_PER_ROUND)\n",
        "federated_train_data = make_federated_data(BAL4_train, sample_clients)\n",
        "\n",
        "state, metrics = iterative_process.next(state, federated_train_data)\n",
        "print('round 1, metrics={}'.format(metrics))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "round 1, metrics=<num_examples=5500.0,loss=1.5703060626983643,accuracy=0.4880000054836273>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkoGLYGLyYkC",
        "colab_type": "code",
        "outputId": "5e28389d-6b3a-4c5f-bfd0-638e37c4537c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "source": [
        "#计算更多轮\n",
        "NUM_ROUNDS = 30\n",
        "for round_num in range(2,NUM_ROUNDS):\n",
        "  #随机选择NUM_CLIENTS个用户\n",
        "  # sample_clients = random.sample(emnist_train.client_ids, NUM_CLIENTS_PER_ROUND)\n",
        "  # federated_train_data = make_federated_data(emnist_train, sample_clients)\n",
        "  \n",
        "  # #BAL1数据集测试\n",
        "  # sample_clients = random.sample(BAL1_train.client_ids, NUM_CLIENTS_PER_ROUND)\n",
        "  # federated_train_data = make_federated_data(BAL1_train, sample_clients)\n",
        "\n",
        "  # #BAL2数据集测试\n",
        "  # sample_clients = random.sample(BAL2_train.client_ids, NUM_CLIENTS_PER_ROUND)\n",
        "  # federated_train_data = make_federated_data(BAL2_train, sample_clients)\n",
        "\n",
        "  # #BAL3数据集测试\n",
        "  # sample_clients = random.sample(BAL3_train.client_ids, NUM_CLIENTS_PER_ROUND)\n",
        "  # federated_train_data = make_federated_data(BAL3_train, sample_clients)\n",
        "\n",
        "  #BAL4数据集测试\n",
        "  sample_clients = random.sample(BAL4_train.client_ids, NUM_CLIENTS_PER_ROUND)\n",
        "  federated_train_data = make_federated_data(BAL4_train, sample_clients)\n",
        "\n",
        "  state, metrics = iterative_process.next(state, federated_train_data)\n",
        "  print('round {:2d}, metrics={}'.format(round_num, metrics))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "round  2, metrics=<num_examples=5500.0,loss=0.7778187990188599,accuracy=0.7709090709686279>\n",
            "round  3, metrics=<num_examples=6100.0,loss=0.5549229383468628,accuracy=0.8381966948509216>\n",
            "round  4, metrics=<num_examples=5310.0,loss=0.32479310035705566,accuracy=0.9048964381217957>\n",
            "round  5, metrics=<num_examples=5300.0,loss=0.2686326205730438,accuracy=0.9203773736953735>\n",
            "round  6, metrics=<num_examples=4300.0,loss=0.25301772356033325,accuracy=0.9286046624183655>\n",
            "round  7, metrics=<num_examples=5000.0,loss=0.1827208250761032,accuracy=0.9473999738693237>\n",
            "round  8, metrics=<num_examples=4900.0,loss=0.20307283103466034,accuracy=0.9448979496955872>\n",
            "round  9, metrics=<num_examples=5050.0,loss=0.12812447547912598,accuracy=0.9675247669219971>\n",
            "round 10, metrics=<num_examples=4050.0,loss=0.1152687817811966,accuracy=0.9691358208656311>\n",
            "round 11, metrics=<num_examples=4600.0,loss=0.127774178981781,accuracy=0.9656521677970886>\n",
            "round 12, metrics=<num_examples=4700.0,loss=0.11916214227676392,accuracy=0.9693617224693298>\n",
            "round 13, metrics=<num_examples=5100.0,loss=0.17643119394779205,accuracy=0.9541176557540894>\n",
            "round 14, metrics=<num_examples=4045.0,loss=0.11449973285198212,accuracy=0.9653893709182739>\n",
            "round 15, metrics=<num_examples=4550.0,loss=0.09060656279325485,accuracy=0.9756044149398804>\n",
            "round 16, metrics=<num_examples=4550.0,loss=0.13460899889469147,accuracy=0.9674725532531738>\n",
            "round 17, metrics=<num_examples=4600.0,loss=0.06949605792760849,accuracy=0.9841304421424866>\n",
            "round 18, metrics=<num_examples=4240.0,loss=0.07007277011871338,accuracy=0.9827830195426941>\n",
            "round 19, metrics=<num_examples=4960.0,loss=0.08050485700368881,accuracy=0.9816532135009766>\n",
            "round 20, metrics=<num_examples=4740.0,loss=0.1020253375172615,accuracy=0.9732067584991455>\n",
            "round 21, metrics=<num_examples=4100.0,loss=0.09300975501537323,accuracy=0.9704878330230713>\n",
            "round 22, metrics=<num_examples=4250.0,loss=0.12350375950336456,accuracy=0.9696470499038696>\n",
            "round 23, metrics=<num_examples=4850.0,loss=0.07014615833759308,accuracy=0.9787628650665283>\n",
            "round 24, metrics=<num_examples=4850.0,loss=0.08621872961521149,accuracy=0.9785566926002502>\n",
            "round 25, metrics=<num_examples=5000.0,loss=0.10374119877815247,accuracy=0.9721999764442444>\n",
            "round 26, metrics=<num_examples=4600.0,loss=0.07180362939834595,accuracy=0.9830434918403625>\n",
            "round 27, metrics=<num_examples=4750.0,loss=0.07463840395212173,accuracy=0.9804210662841797>\n",
            "round 28, metrics=<num_examples=5600.0,loss=0.04994412511587143,accuracy=0.9878571629524231>\n",
            "round 29, metrics=<num_examples=5050.0,loss=0.06198875978589058,accuracy=0.9847524762153625>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_EV-vx8b93f",
        "colab_type": "text"
      },
      "source": [
        "### 使用TensorBoard可视化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKKb6c6h0AMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#使用tensorboard可视化\n",
        "#使用Tensorboard可视化这些联邦计算的度量\n",
        "#创建目录和相应的摘要编写器\n",
        "#@test {\"skip\": true}\n",
        "logdir = \"/tmp/logs/scalars/training/\"\n",
        "summary_writer = tf.summary.create_file_writer(logdir)\n",
        "state = iterative_process.initialize()\n",
        "\n",
        "#!!!太坑了，我以为@test是无关紧要的东西，emmm，就省略了，没想到就凉凉\n",
        "#@test {\"skip\": true}\n",
        "with summary_writer.as_default():\n",
        "  for round_num in range(1, NUM_ROUNDS):\n",
        "    #随机选择用户\n",
        "    sample_clients = random.sample(emnist_train.client_ids, NUM_CLIENTS)\n",
        "    federated_train_data = make_federated_data(emnist_train, sample_clients)\n",
        "\n",
        "    state, metrics = iterative_process.next(state, federated_train_data)\n",
        "\n",
        "    print('round {:2d}, metrics={}'.format(round_num, metrics))\n",
        "\n",
        "    for name, value in metrics._asdict().items():\n",
        "      tf.summary.scalar(name, value, step=round_num)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn70ix21PgDn",
        "colab_type": "code",
        "outputId": "117f75c1-645d-482c-c8f0-e7c1188cf29b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "source": [
        "'''\n",
        "#这个tensorBoard像有猫病一样，昨天还可以今天来突然就不行了\n",
        "#@test {\"skip\":true}\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /tmp/logs/scalars/ --port=0\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ERROR: Failed to launch TensorBoard (exited with 1).\n",
              "Contents of stderr:\n",
              "2020-04-06 08:24:12.865933: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
              "2020-04-06 08:24:12.866088: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
              "2020-04-06 08:24:12.866105: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
              "Traceback (most recent call last):\n",
              "  File \"/usr/local/bin/tensorboard\", line 8, in <module>\n",
              "    sys.exit(run_main())\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/main.py\", line 66, in run_main\n",
              "    app.run(tensorboard.main, flags_parser=tensorboard.configure)\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\n",
              "    _run_main(main, args)\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\n",
              "    sys.exit(main(argv))\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/program.py\", line 268, in main\n",
              "    return runner(self.flags) or 0\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/program.py\", line 282, in _run_serve_subcommand\n",
              "    server = self._make_server()\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/program.py\", line 361, in _make_server\n",
              "    self.assets_zip_provider)\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/backend/application.py\", line 165, in standard_tensorboard_wsgi\n",
              "    flags, plugin_loaders, data_provider, assets_zip_provider, multiplexer)\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/backend/application.py\", line 234, in TensorBoardWSGIApp\n",
              "    return TensorBoardWSGI(tbplugins, flags.path_prefix)\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/backend/application.py\", line 286, in __init__\n",
              "    raise ValueError('Duplicate plugins for name %s' % plugin.plugin_name)\n",
              "ValueError: Duplicate plugins for name whatif"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHsGUj-3b07z",
        "colab_type": "text"
      },
      "source": [
        "### 在测试集上评估"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBGzrtDgP8hn",
        "colab_type": "code",
        "outputId": "3063facd-d1ca-44d4-9862-84725d1edb2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "#在测试集上进行测试\n",
        "#求值传递MnistModel就足够了，不执行梯度下降，也不需要构造优化器，MnistTModel就是前面定义的那个class\n",
        "evaluation = tff.learning.build_federated_evaluation(MnistModel)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pool2 Tensor(\"MaxPool2d_1:0\", shape=(None, 7, 7, 64), dtype=float32)\n",
            "flatten over\n",
            "dense1 over\n",
            "dense2 over\n",
            "y_pred Tensor(\"Softmax:0\", shape=(None, 10), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQjEUho65VGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#评估一下在训练中得到的最新状态，为了从服务器状态中提取最新的训练模型，只需访问.model成员\n",
        "#和上面说的形式一样，它接收model和联邦数据并返回训练的metrics\n",
        "#这里是在评估模型的最新状态，为了从服务器状态中提取最新的训练模型，只需访问.model成员\n",
        "train_metrics = evaluation(state.model, federated_train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M5UwerU5V6R",
        "colab_type": "code",
        "outputId": "8a1d6ccb-6a60-4510-b36f-70bf15a474d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#编译联邦数据的测试样本，并对测试数据重新运行求值。同样也是随机选择NUM_CLIENTS个用户。\n",
        "# sample_clients = random.sample(emnist_train.client_ids, NUM_CLIENTS_PER_ROUND)\n",
        "# federated_test_data = make_federated_data(emnist_test, sample_clients)\n",
        "\n",
        "# #BAL1测试精度\n",
        "# sample_clients = random.sample(BAL1_test.client_ids, NUM_CLIENTS_PER_ROUND)\n",
        "# federated_test_data = make_federated_data(BAL1_test, sample_clients)\n",
        "# len(federated_test_data), federated_test_data[0]\n",
        "\n",
        "# #BAL2测试精度\n",
        "# sample_clients = random.sample(BAL2_test.client_ids, NUM_CLIENTS_PER_ROUND)\n",
        "# federated_test_data = make_federated_data(BAL2_test, sample_clients)\n",
        "# len(federated_test_data), federated_test_data[0]\n",
        "\n",
        "# #BAL3测试精度\n",
        "# sample_clients = random.sample(BAL3_test.client_ids, NUM_CLIENTS_PER_ROUND)\n",
        "# federated_test_data = make_federated_data(BAL3_test, sample_clients)\n",
        "# len(federated_test_data), federated_test_data[0]\n",
        "\n",
        "#BAL4测试精度\n",
        "sample_clients = random.sample(BAL4_test.client_ids, NUM_CLIENTS_PER_ROUND)\n",
        "federated_test_data = make_federated_data(BAL4_test, sample_clients)\n",
        "len(federated_test_data), federated_test_data[0]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8,\n",
              " <PrefetchDataset shapes: OrderedDict([(x, (None, 28, 28, 1)), (y, (None, 1))]), types: OrderedDict([(x, tf.float32), (y, tf.int32)])>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a_F78e05cKM",
        "colab_type": "code",
        "outputId": "72c3a12e-7bf3-4fe8-f41c-6ea33d0b5d53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#使用EMNIST，客户端随机选择10个，初始精度在0.8898\n",
        "#使用BAL1，K=20,客户端随机选了5个作为测试集测试，训练30轮，精度在0.9616666436195374\n",
        "\n",
        "# #使用BAL1，K=375，C=0.02，训练30轮，测试精度0.9637500047683716\n",
        "# test_metrics = evaluation(state.model, federated_test_data)\n",
        "\n",
        "# #使用BAL2，K=375，C=0.02，训练30轮，测试精度0.956250011920929\n",
        "# test_metrics = evaluation(state.model, federated_test_data)\n",
        "\n",
        "# #使用BAL3，K=375，C=0.02，训练30轮，测试精度0.949400782585144\n",
        "# test_metrics = evaluation(state.model, federated_test_data)\n",
        "\n",
        "#使用BAL4，K=375，C=0.02，训练30轮，测试精度0.9477777481079102\n",
        "test_metrics = evaluation(state.model, federated_test_data)\n",
        "\n",
        "str(test_metrics)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<num_examples=4500.0,loss=0.16900211572647095,accuracy=0.9477777481079102>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}